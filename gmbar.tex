%\documentclass[aip,jcp,reprint,amsmath,amssymb]{revtex4-1}
\documentclass[aip,jcp,preprint,amsmath,amssymb]{revtex4-1}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[pdftex]{graphicx}

\newcommand{\mt}[1]{\boldsymbol{\mathbf{#1}}}           % matrix symbol
\newcommand{\vt}[1]{\boldsymbol{\mathbf{#1}}}           % vector symbol
\newcommand{\tr}[1]{#1^\text{t}}                        % transposition
\newcommand{\diff}[2]{\frac{\partial #1}{\partial #2}}  % partial derivative

\begin{document}

\title{A Generalization of the Multistate Bennett Acceptance Ratio Method}

\author{Charlles R. A. Abreu}
\email{abreu@eq.ufrj.br}
\affiliation{Chemical Engineering Department, Escola de Quimica, Universidade Federal do Rio de Janeiro, Rio de Janeiro, RJ 21941-909, Brazil}

\date{\today}

\maketitle

\section{Definitions}
\label{sec:definitions}

Consider that $x$ represents the set of coordinates of a system that can be found at different equilibrium states. Each state $i$ is a statistical ensemble distinguished by a conditional probability density
\begin{equation}
\label{eq:state_prob_density}
\rho_i(x) = \frac{1}{Z_i} e^{-u_i(x)},
\end{equation}
where $u_i(x)$ is a \textit{reduced potential}\cite{Shirts_2008, Chodera_2011} with known functional form and $Z_i = \int e^{-u_i(x)}dx$ is an unknown normalization constant, referred to as the configurational integral or partition function of the system at state $i$. The ensemble average or equilibrium expectation of a property $A(x)$ at state $i$ is defined as
\begin{equation}
\label{eq:ensemble-average}
\langle A \rangle_i = \int A(x)\rho_i(x)dx.
\end{equation}

Assume that $n_i$ configurations $\{x_{i,k}\}_{k=1}^{n_i}$ were obtained via importance sampling\cite{Allen_1987} (e.g. Monte Carlo or Molecular Dynamics) at state $i$. In this case, we can estimate $\langle A \rangle_i$ by a simple arithmetic average, that is,
\begin{equation}
\label{eq:average_estimator}
\bar A_i = \frac{1}{n_i} \sum_{k=1}^{n_i} A(x_{i,k}).
\end{equation}

We are often interested in computing the difference between the free energies of two states $i$ and $j$, defined as $\Delta f_{ij} = f_j - f_i = - \ln (Z_j/Z_i)$. Because it is usually unfeasible to compute absolute free energies, we find it appropriate to define $f_i$ as a relative value with respect to a state $0$, chosen as a common reference to all other states of interest, which makes $f_i = -\ln (Z_i/Z_0)$. This allows us to rewrite Eq.~\eqref{eq:state_prob_density} as
\begin{equation}
\label{eq:state_prob_density_Z0}
\rho_i(x) = \frac{1}{Z_0} e^{-u_i(x)+ f_i}.
\end{equation}

Estimation of $f_i$ can be done by relating it to one or more ensemble averages. However, we must remark that the tails of $\rho_i(x)$ are usually overlooked in importance sampling, meaning that only a limited region $C_i$ of the configurational space is sampled with reasonable accuracy. As in Ref.~\onlinecite{Jarzynski_2006}, we refer to $C_i$ as the set of \textit{typical} configurations of state $i$. It does not necessarily coincide with the set $D_i$ of configurations that substantially contribute to the integral in Eq.~\eqref{eq:ensemble-average}, referred to as the \textit{dominant} configurations. As a matted of fact, the estimator described in Eq.~\eqref{eq:average_estimator} will be accurate only if $D_i$ is a significant subset of $C_i$. This issue is particularly important in the calculation of free-energy differences and their uncertainties.

\section{Free-Energy Computation Methods}

Traditional methods such as \textit{free energy perturbation} (FEP),\cite{Zwanzig_1954} \textit{simple overlap sampling},\cite{Lee_1980, Lu_2003} and \textit{Bennett's acceptance ratio} (BAR)\cite{Bennett_1976} are pairwise with respect to the calculation of free-energy differences between sampled states. These are computed independently for the pairs of states whose sets of typical configurations overlap most considerably. The additive nature of this property then allows us to piece these values together and obtain free-energy differences for poorly- or non-overlapping pairs of states. This strategy is known as \textit{staging}.\cite{Kofke_1998} Among the cited methods, FEP is the least recommended one. For states $i$ and $j$, it consists in evaluating either $\Delta f_{ij} = -\ln \langle e^{u_i - u_j} \rangle_i$ or $\Delta f_{ij} = \ln \langle e^{u_j - u_i} \rangle_j$.\cite{Zwanzig_1954} It happens, however, that these averages entail dominant sets $D_i$ and $D_j$ that practically coincide with the typical sets $C_j$ and $C_i$, respectively.\cite{Jarzynski_2006} Note that this is opposite to the situation that would maximize accuracy. On the other hand, BAR is the most accurate method because it manages, in an iterative way, to relate $\Delta f_{ij}$ to ensemble averages computed at states $i$ and $j$ whose dominant sets $D_i$ and $D_j$ fulfill the intersection of $C_i$ and $C_j$.

Another way of tackling the issue of overlap deficiency consists in defining a reference state, with probability density $\rho_0(x) = \frac{1}{Z_0}e^{-u_0(x)}$, whose typical configurations form a contiguous set $C_0$ that encompasses all sets $\{C_i\}_{i=1}^m$ associated to a number $m$ of states of interest. In this case, instead of sampling configurations at these states, we do it at state $0$ and then compute every $f_i$ by employing the perturbation formula
\begin{equation}
\label{eq:umbrella sampling free energy}
f_i = -\ln \langle e^{u_0-u_i} \rangle_0.
\end{equation}

As explained above, the dominant set $D_0$ will coincide with $C_i$ and thus be a subset of $C_0$. Torrie and Valleau\cite{Torrie_1977} pioneered this idea, for which they coined the term \textit{umbrella sampling}. The method is also known as \textit{non-Boltzmann sampling} due to the usually unphysical character of the sampled state. In addition to the relative free energies, one is able to compute ensemble averages of any property $A(x)$ at the states of interest by properly reweighting the configurations sampled at state $0$. This is done by means of the identity\cite{Torrie_1977}
\begin{equation}
\label{eq:umbrella sampling reweighting}
\langle A \rangle_i = \frac{\langle A e^{u_0 - u_i} \rangle_0}{\langle e^{u_0 - u_i} \rangle_0} = \langle A e^{u_0 - u_i + f_i} \rangle_0.
\end{equation}

The multicanonical approach\cite{Berg_1992, Lee_1993, Abreu_2006} and the enveloping distribution method\cite{Christ_2007, *Christ_2008, *Christ_2009} are examples of successful non-Boltzmann sampling techniques. One of particular importance to the train of thought we intend to develop here is the method of \textit{expanded ensembles}.\cite{Lyubartsev_1992} As described in the subsection that follows, it is a non-Boltzmann sampling scheme closely related to staging.

\subsection{Expanded Ensembles}

The method consists in simulating a combination of several states whose individual distributions overlap, but performing a random walk through such states instead of sampling them independently. The joint probability of configurations and states is given by\cite{Nymeyer_2010}
\begin{equation}
\label{eq:expanded ensemble joint}
p_0(x, i) = \frac{1}{Z_0} e^{-u_i(x) + \eta_i},
\end{equation}
where $\eta_i$ is an arbitrary weighting factor. The marginal probability $\pi_i$ of each state $i$ is obtained by integrating $p_0(x,i)$ over the configurational space. One can also obtain the marginal probability density $\rho_0(x)$ by summing $p_0(x,i)$ for all $m$ states. These procedures yield
\begin{align}
\pi_i = \frac{Z_i}{Z_0} e^{\eta_i} = e^{\eta_i - f_i} \quad \text{and} \label{eq:expanded ensemble prior} \\
\rho_0(x) = \frac{1}{Z_0} \sum_{j=1}^m e^{-u_j(x) + \eta_j}. \label{eq:expanded ensemble evidence}
\end{align}

Once the weighting factors $\{\eta_i\}_{i=1}^m$ are defined, there are several ways of performing importance sampling from the probability in Eq.~\eqref{eq:expanded ensemble joint}. For instance, one can use it in a Monte Carlo simulation that involves both configurational and state-changing moves.\cite{Lyubartsev_1992} This will result in a sample $S = \{x_k,i_k\}_{k=1}^n$, with each configuration bearing a state-indicating label. Another way is via Monte Carlo (MC) or Molecular Dynamics (MD) simulation of a system whose reduced potential is
\begin{equation}
\label{eq:expanded ensemble potential}
u_0(x) = - \ln \sum_{j=1}^m e^{-u_j(x) + \eta_j},
\end{equation}
which results in the probability density of Eq.~\eqref{eq:expanded ensemble evidence}. In this case, the sampled configurations will not bear any labels at first. If these labels are required for any reason, we can randomly pick one for each configuration from the conditional probability distribution $p(i|x)$.\cite{Nymeyer_2010} From Bayes' theorem $p(x,i) = p(x|i) p(i) = p(i|x) p(x)$, we can divide Eq.~\eqref{eq:expanded ensemble joint} by Eq.~\eqref{eq:expanded ensemble evidence} to find out that
\begin{equation}
\label{eq:expanded ensemble posterior}
p(i|x) = \frac{e^{-u_i(x) + \eta_i}}{\sum_{j=1}^m e^{-u_j(x) + \eta_j}}.
\end{equation}

In addition, we can also divide Eq.~\eqref{eq:expanded ensemble joint} by Eq.~\eqref{eq:expanded ensemble prior} to see how configurations are distributed within each state. The results is $p(x|i) = \frac{1}{Z_0} e^{u_i(x) + f_i}$, which is identical to Eq.~\eqref{eq:state_prob_density_Z0} and proves that each state of the expanded ensemble is sampled correctly. The method proposed by Nymeyer\cite{Nymeyer_2010} and a special case of the one devised by Christ and van Gunsteren\cite{Christ_2007, *Christ_2008, *Christ_2009} are examples of using Eq.~\eqref{eq:expanded ensemble potential} to perform non-Boltzmann sampling with Molecular Dynamics. A third method for sampling from an expanded ensemble is known as \textit{Gibbs sampling}.\cite{Chodera_2011} While the state $i$ is kept fixed, configurations can be sampled from $p(x|i)$ via MC or MD. Also, while keeping the configuration $x$ fixed, a state can be chosen according to $p(i|x)$. Alternating these procedures will produce the correct expanded ensemble distribution.\cite{Chodera_2011}

Once a sample $S$ has been generated, a simple estimator for the relative free energy of each state stems from Eq.~\eqref{eq:expanded ensemble prior} by making $\hat \pi_i = n_i/n$, where $n_i$ is the number of appearances of state $i$ in $S$. The result is
\begin{equation}
\label{eq:expanded ensemble histogram estimator}
\hat f_i = \eta_i - \ln \frac{n_i}{n}.
\end{equation}

However, a better estimator can be obtained by combining Eqs.~\eqref{eq:umbrella sampling free energy} and \eqref{eq:expanded ensemble potential} and then using Eq.~\eqref{eq:average_estimator} to compute the required ensemble average. It is
\begin{equation}
\label{eq:expanded ensemble FEP estimator}
\hat f_i = -\ln \left[ \frac{1}{n}\sum_{k=1}^n \frac{e^{-u_i(x_k)}}{\sum_{j=1}^m e^{-u_j(x_k) + \eta_j}} \right].
\end{equation}

Both estimators neglect the detailed association of configurations to states. This is due to the essence of importance sampling, which implies giving the same weight to every sampled configuration. One may note that the second estimator requires evaluating all reduced potentials for every $x_k \in S$. It occurs that, when sampling is done for a system with many degrees of freedom, it is a common practice to store only the values of some collective properties, rather than all configurational details, for posterior analysis. In this event, unless closed-form relations exist among the different potentials, one must compute and store all of them during the sampling in order to be able to evaluate Eq.~\eqref{eq:expanded ensemble FEP estimator} in a post-processing step. Note that this is mandatory only if the potential of Eq.~\eqref{eq:expanded ensemble potential} is used. Finally, we remark that the set of functions $\{u_i(x)\}_{i=1}^m$ is often considered as a given and the weights are set so that all states are visited with equal frequency, thus producing a flat histogram. By replacing $\pi_i = 1/m$ in Eq.~\eqref{eq:expanded ensemble prior}, we conclude that $\eta_i = f_i - \ln m$ must hold for all $i$ (up to a common additive constant) for this to occur, which can only be achieved iteratively. Other strategies have also been proposed to select suitable weights or suitable reduced potentials, aiming at improving performance and avoiding ergodicity issues. For details, we call the reader's attention to Refs.~\onlinecite{Katzgraber_2006, Trebst_2006, Escobedo_2007, *Escobedo_2008, *Martinez_2008}.

\subsection{Multistate Bennett Acceptance Ratio Method}

An interesting feature of the method of expanded ensembles is that the walk through states favors ergodicity in each state, as compared to sampling each one individually. Nevertheless, the process of determining convenient states and weights can be tedious and time-consuming. It is more likely that we face the situation described in Sec.~\ref{sec:definitions}, in which samples individually drawn at different states are available. An advantage of this scenario is that inclusion of new states, if necessary, is straightforward. In this case, the pooled sample can be expressed as
\begin{equation}
\label{eq:pooled sample}
\mathcal S = \Big\{\{x_{i,k}\}_{k=1}^{n_i}\Big\}_{i=1}^m.
\end{equation}

Shirts and Chodera\cite{Shirts_2008} described a method for computing relative free energies and their uncertainties by using all configurations in $\mathcal S$. Because it is equivalent to BAR when only two states are involved, the authors named it as Multistate Bennet Acceptance Ratio (MBAR). Before embarking in a detailed analysis, let us derive their estimator from a simple argument. MBAR consists in assigning the same importance to every configuration, regardless of which state it comes from. We can thus overlook the subset structure of Eq.~\eqref{eq:pooled sample} and write $\mathcal S = \{x_k\}_{k=1}^n$. This is equivalent to viewing $\mathcal S$ as the outcome of an expanded ensemble simulation whose weighting factors are unknown and must be estimated as well. For this reason, we are compelled to use both estimators in Eqs.~\eqref{eq:expanded ensemble histogram estimator} and \eqref{eq:expanded ensemble FEP estimator}, rather than choosing one of them. A combination of these estimators yield
\begin{equation}
\label{eq:mbar free energy estimator}
\hat f_i = -\ln \sum_{k=1}^n \frac{e^{-u_i(x_k)}}{\sum_{j=1}^m n_j e^{-u_j(x_k) + \hat f_j}},
\end{equation}
which is equivalent to Eq.~11 of Ref.~\onlinecite{Shirts_2008}. Note that this is not an isolated estimator, but a self-consistent system of equations whose solution provides a whole set of estimators $\{\hat f_i\}_{i=1}^m$. In fact, the solution can only be determined up to an additive constant and, as usual, one can fix $\hat f_1 = 0$ and solve the remaining equations in order to obtain $\{\hat f_i\}_{i=2}^m$. This means is the same as defining the reference state in such a way that $Z_0 = Z_1$.

As the $m$ samples in $\mathcal S$ are generated independently, giving all configurations the same importance seems unsuitable if these samples consist of correlated time series with distinct mixing times. This is one reason why MBAR is meant to deal with uncorrelated data and, as such, requires subsampling if the series obtained at any state is originally correlated.\cite{Shirts_2008} For a series $\{x_{i,k}\}_{k=1}^{n_i}$ originally drawn at state $i$, the subsampling interval should be set as the statistical inefficiency of some configurational property such as, for instance, the reduced potential $u_i(x)$. The statistical inefficiency of a property $A(x)$ can be computed as\cite{Chodera_2007}
\begin{equation}
\label{eq:statistical inefficiency}
g_i = 1 + 2 \sum\limits_{t=1}^{n_i-1} \frac{n_i - t}{n_i} C_i(t),
\end{equation}
where $C_i(t)$ is the normalized autocorrelation function of $A(x)$, calculated by
\begin{equation*}
C_i(t) = \frac{\dfrac{1}{n_i - t} \sum\limits_{k=1}^{n_i-t} [A(x_{i,k}) - \bar A_i][A(x_{i,k+t}) - \bar A_i]}{\dfrac{1}{n_i} \sum\limits_{k=1}^{n_i} [A(x_{i,k}) - \bar A_i]^2}.
\end{equation*}

Ideally, one should compute $g_i$ for a number of relevant properties and take the largest one as the subsampling interval.\cite{Shirts_2008} The autocorrelation function $C_i(t)$ is expected to asymptotically decay to zero for most properties, but in practice its tail fluctuates around zero. Chodera \textit{et al}.\cite{Chodera_2007} propose that the sum in Eq.~\eqref{eq:statistical inefficiency} be truncated as soon as $C_i(t)$ crosses zero for the first time, arguing that the function becomes statistically indistinguishable from zero at this point.

Another reason why MBAR is applied exclusively to uncorrelated data is concerned with the determination of uncertainties. The MBAR estimator originated in the literature of Statistical Inference, where it appeared under other denominations such as Biased Sampling,\cite{Vardi_1985, *Gill_1988} Reverse Logistic Regression,\cite{Geyer_1994} and Extended Bridge Sampling.\cite{Meng_1996, Kong_2003, Tan_2004} Geyer\cite{Geyer_1994} derived the method as the 



derived the estimator from the principle of Maximum Likelihood,\cite{Pawitan_2001, Greene_2012} which unveils some important properties 

such as \textit{consistency} (convergence to ) and \textit{asymptotic normality} (i.e. a proper central limit theorem) in the case of uncorrelated samples.


Two distinct approacheshave been used to derive it from the principle of Maximum Likelihood


, introduced by Geyer in Ref.~\onlinecite{Geyer_1994} and by Kong \textit{et al}. in Ref.~\onlinecite{Kong_2003}, .



Geyer\cite{Geyer_1994} As it is possible to derive Eq.~\eqref{eq:mbar free energy estimator} from the principle of Maximum Likelihood,


The same estimator originated in the literature of Statistical Inference under different denominations, such as Biased Sampling,\cite{Vardi_1985, *Gill_1988} Reverse Logistic Regression,\cite{Geyer_1994} and Extended Bridge Sampling.\cite{Meng_1996, Kong_2003, Tan_2004} 



At least two distinct approaches, introduced by Geyer in Ref.~\onlinecite{Geyer_1994} and by Kong \textit{et al}. in Ref.~\onlinecite{Kong_2003}, can be used to derive it from the principle of Maximum Likelihood.

\begin{equation}
\mathcal L = \prod_{i=1}^m \prod_{k=1}^{n_i} p(i|x_{i,k})
\end{equation}


It is regarded as a Maximum Likelihood Estimator (MLE)

\section{DRAFT}

A set of equations that link the relative free energies to ensemble averages, but are devoid of such weights, can be obtained from Eqs.~\eqref{eq:umbrella sampling free energy}, \eqref{eq:expanded ensemble prior}, and \eqref{eq:expanded ensemble potential}. It is, for all $i$ from $1$ to $m$,
\begin{equation}
\label{eq:mbar free energy exact}
f_i = -\ln \left\langle \frac{e^{-u_i}}{\sum_{j=1}^m \pi_j e^{-u_j + f_j}} \right\rangle_0.
\end{equation}

Following this reasoning, the same importance is assigned \textit{a priori} to every configuration in the pooled sample $\mathcal S$, that is, 


. This makes $\pi_i = n_i/n$, where $n = \sum_{i=1}^m n_i$, since each configuration 



This renders the association of a state to each configuration irrelevant, so that we can overlook the subset structure of Eq.~\eqref{eq:pooled sample} and write $\mathcal S = \{x_k\}_{k=1}^n$. The only information 

The prior probability of each state is proportional to 

makes $\pi_i = n_i/n$, where $n = \sum_{i=1}^m n_i$, and renders the association of configurations to states irrelevant. We can thus overlook the subset structure of Eq.~\eqref{eq:pooled sample} and write $\mathcal S = \{x_k\}_{k=1}^n$. Finally, the estimator that stems from Eq.~\eqref{eq:mbar free energy exact} is
\begin{equation}
\label{eq:mbar free energy estimator}
\hat f_i = -\ln \sum_{k=1}^n \frac{e^{-u_i(x_k)}}{\sum_{j=1}^m n_j e^{-u_j(x_k) + \hat f_j}}.
\end{equation}

This expression, which is equivalent to Eq.~11 of Ref.~\onlinecite{Shirts_2008}, could have also been obtained directly from the estimators in Eqs.~\eqref{eq:expanded ensemble histogram estimator} and \eqref{eq:expanded ensemble FEP estimator}. Some of its features deserve attention. First, it is not an isolated estimator, but a self-consistent system of equations whose solution estimates the whole set $\{f_i\}_{i=1}^m$. Second, it is 

Log-quasi-likelihood:\cite{Geyer_1994}
\begin{equation*}
\ln \mathcal L = \sum_{k=1}^n \ln p(i_k|x_k) = \sum_{k=1}^n -u_i(x) + \eta_i - \ln \sum_{j=1}^m e^{-u_j(x) + \eta_j}.
\end{equation*}

\begin{equation}
\label{eq:mbar log quasi-likelihood}
\ln \mathcal L = \frac{1}{n} \sum_{k=1}^m \sum_{l=1}^{n_k} \ln p(k|x_{k,l}),
\end{equation}
where
\begin{equation*}
\ln p(k|x) = -u_k(x) + f_k + \ln \pi_k - \ln \sum_{s=1}^m \pi_s e^{-u_s(x) + f_s}.
\end{equation*}


\section{A Generalization of MBAR}

Eqs.~\eqref{eq:expanded ensemble prior}, \eqref{eq:expanded ensemble evidence}, and \eqref{eq:state_prob_density_Z0} can be combined to give
\begin{equation*}
\rho_0(x) = \frac{1}{Z_0} \sum_{i=1}^m \pi_i e^{-u_i(x) + f_i} = \sum_{i=1}^m \pi_i \rho_i(x).
\end{equation*}

This shows that $\rho_0(x)$ is a mixture of the probability densities of all states. Therefore, an ensemble average at state $0$ is
\begin{equation*}
\langle A \rangle_0 = \sum_{i=1}^m \pi_i \langle A \rangle_i.
\end{equation*}

Thus, from Eq.~\eqref{eq:mbar free energy exact}:
\begin{equation}
f_i = -\ln \sum_{j=1}^m \pi_j \left\langle \frac{e^{-u_i}}{\sum_{s=1}^m \pi_s e^{-u_s + f_s}} \right\rangle_j.
\end{equation}

GMBAR Estimator:
\begin{equation}
\hat f_i = -\ln \sum_{j=1}^m \frac{\pi_j}{n_j} \sum_{k=1}^{n_j} \frac{e^{-u_i(x_{j,k})}}{\sum_{s=1}^m \pi_s e^{-u_s(x_{j,k}) + \hat f_s}}.
\end{equation}


Definition 1: $p_i(x)$ is the probability of state $i$ for a given configuration $x$
\begin{equation}
p_i(x) = p(i|x) = \frac{\pi_i e^{-u_i(x) + f_i}}{\sum_{s=1}^m \pi_s e^{-u_s(x) + \hat f_s}}
\end{equation}

Definition 2: $M_{ij}$ is the average probability of state $i$ for all configurations sampled at state $j$:
\begin{equation*}
M_{ij} = \langle p_i \rangle_j \approx \frac{1}{n_j} \sum_{k=1}^{n_j} p_i(x_{jk})
\end{equation*}

Therefore, $\vt M$ is a stochastic matrix and, consequently, the Perron-Frobenius theorem applies. GMBAR in matrix notation:
\begin{equation*}
\mt M \vt \pi = \vt \pi
\end{equation*}

Thus, we have to look for a matrix $\mt M$ whose limiting distribution is $\vt \pi$.

\subsection{Maximum Quasi-Likelihood Derivation}

Log quasi-likelihood function\cite{Geyer_1992, Geyer_1994}
\begin{equation}
\label{eq:gmbar log quasi-likelihood}
\ln \mathcal L = \sum_{k=1}^m \frac{\pi_k}{n_k} \sum_{l=1}^{n_k} \ln p(k|x_{k,l}),
\end{equation}
where
\begin{equation*}
\ln p(k|x) = -u_k(x) + f_k + \ln \pi_k - \ln \sum_{s=1}^m \pi_s e^{-u_s(x) + f_s}.
\end{equation*}

By straightforward differentiation, we can obtain the first and second derivatives of $\ln p(k|x)$ with respect to $f_i$ and $f_j$, which are
\begin{gather*}
\frac{\partial \ln p(k|x)}{\partial f_i} = \delta_{i,k} - p(i|x) \quad \text{and} \\
\frac{\partial^2\ln p(k|x)}{\partial f_i \partial f_j} = -p(i|x)[\delta_{i,j} - p(j|x)],
\end{gather*}
where $\delta_{i,j}$ is the Kronecker delta. From these derivatives, we find out that
\begin{equation*}
s_i = \frac{\partial \ln \mathcal L}{\partial f_i} = \pi_i - \sum_{k=1}^m M_{i,k} \pi_k,
\end{equation*}
where $M_{i,k}$ is the mean posterior probability of state $i$ among the configurations sampled at state $k$, that is,
\begin{equation*}
M_{i,k} = \frac{1}{n_k} \sum_{l=1}^{n_k} p(i|x_{k,l}).
\end{equation*}

Note that $\mt M$ is the overlap matrix defined in Ref.~\onlinecite{Klimovich_2015}.

If we define an information matrix $\mt B$ as the reciprocal of the Hessian matrix of $\ln \mathcal L$ with respect to the relative free energies, so that $B_{i,j} = -\frac{\partial^2 \ln \mathcal L}{\partial f_i \partial f_j}$, we have
\begin{equation*}
B_{i,j} = \delta_{i,j} \sum_{k=1}^m M_{i,k} \pi_k - \sum_{k=1}^m \frac{\pi_k}{n_k} \sum_{l=1}^{n_k} p(i|x_{k,l})p(j|x_{k,l}).
\end{equation*}

In matrix notation:
\begin{equation*}
\mt B = \text{diag}(\mt M \vt \pi) - \sum_{i=1}^m \frac{\pi_i}{n_i} \mt P_i \tr{\mt P}_i,
\end{equation*}
where
\begin{equation*}
\mt \Omega_k = \frac{\mt P_k \tr{\mt P}_k}{n_k}.
\end{equation*}


\begin{equation}
x_{max} + \ln \sum_{i=1}^n a_i e^{x_i -x_{max}}
\end{equation}

Newton-Raphson:
\begin{equation*}
\Delta \vt f
\end{equation*}

\section{Results}

\subsection{Solvation of Propane in Water}

In order to compare the proposed method against MBAR in regard to the calculation of relative free energies of sampled states, we simulated the infinite-dilution solvation of propane in water. Both molecules are modeled as rigid bodies. The intermolecular interactions between water atoms and propane pseudo-atoms are calculated by the softcore potential\cite{Beutler_1994} $\nu(r,\lambda) = 4\lambda\epsilon(s^{-2} - s^{-1})$, where $s = (r/\sigma)^6 + 0.5 (1-\lambda)$.

\begin{table}
\caption{Reduced relative free energies}
\label{table:propane solvation}
\begin{ruledtabular}
\begin{tabular}{ccccc}
$\lambda$ & $g$ & MBAR & GMBAR & Optimized \\
\hline
$0.0$ & $12$ & $0.0$ & $0.0$ & $0.0$ \\
$0.1$ & $14$ & $0.4547 \pm 0.0025$ & $0.4563 \pm 0.0018$ \\
$0.2$ & $21$ & $1.7643 \pm 0.0073$ & $1.7668 \pm 0.0056$ \\
$0.3$ & $27$ &  $3.497 \pm 0.018$  &  $3.491 \pm 0.014$ \\
$0.4$ & $12$ &  $4.475 \pm 0.027$  &  $4.462 \pm 0.020$ \\
$0.5$ & $8$  &  $4.728 \pm 0.030$  &  $4.712 \pm 0.022$ \\
$0.6$ & $5$  &  $4.636 \pm 0.031$  &  $4.619 \pm 0.023$ \\
$0.8$ & $4$  &  $3.979 \pm 0.032$  &  $3.958 \pm 0.023$ \\
$1.0$ & $2$  &  $2.989 \pm 0.032$  &  $2.968 \pm 0.024$
\end{tabular}
\end{ruledtabular}
\end{table}

\begin{figure}
\centering
\includegraphics{Figures/nelder_mead}
\caption{}
\label{fig:nelder_mead}
\end{figure}


\section{DRAFT}

visiting other states with faster mixing will help 

Based on the work of Vardi,\cite{Vardi_1985} Geyer,\cite{Geyer_1994} Meng and Wong,\cite{Meng_1996}, Kong \textit{et al}.\cite{Kong_2003}, and Tan\cite{Tan_2004}

Reweighting mixture\cite{Geyer_1994}:
\begin{equation*}
\rho_0(x) = \frac{1}{Z_0} \sum_{i=1}^{n_0} \pi_i e^{-u_i(x) + f_i}
\end{equation*}

Since $\rho_0 = \frac{1}{Z_0} e^{-u_0}$, the equation above results in the following reduced potential
\begin{equation}
\label{eq:mbar_reduced_potential}
u_0(x) = -\ln \sum_{i=1}^m \pi_i e^{-u_i(x) + f_i}
\end{equation}





The probability density $\rho_i(x)$ can be interpreted as a conditional probability of configuration $x$ given state $i$, i.e. $\rho_i(x) = p(x|i)$. We can join all configurations sampled in states $1$ to $m$ together as if they were sampled from a single state $0$. In this case, $n_0 = \sum_{k=1}^m n_k$. Let us assume the prior probability that a configuration comes from state $i$ as being $p(i) = n_i/n_0$. In this case, the Bayes theorem yields
\begin{equation*}
p(x|i) p(i) = p(i|x) p(x)
\end{equation*}

\begin{equation*}
p_i \rho_i(x) = p(i|x) \rho_0(x)
\end{equation*}

\begin{equation}
\label{eq:mbar_probability_density}
\rho_0(x) = \sum_{i=1}^m p_i \rho_i(x) = \frac{1}{Z_0} \sum_{i=1}^m p_i e^{-u_i(x) + f_i}
\end{equation}

Since $\rho_0 = \frac{1}{Z_0} e^{-u_0}$, the equation above results in the following reduced potential
\begin{equation}
\label{eq:mbar_reduced_potential}
u_0(x) = -\ln \sum_{i=1}^m p_i e^{-u_i(x) + f_i}
\end{equation}

From Eqs.~\eqref{eq:umbrella_sampling_free_energy} and \eqref{eq:average_estimator},
\begin{equation*}
f_i = -\ln \sum_{k=1}^{n_0} \frac{e^{-u_i(x_k)}}{\sum_{j=1}^m n_j e^{-u_j(x_k) + f_j}}
\end{equation*}

An average in the reference state can be rewritten as
\begin{equation}
\label{eq:average_reference_state}
\langle A \rangle_0 = \sum_{i=1}^m p_i \langle A \rangle_i
\end{equation}

Therefore,
\begin{equation*}
\label{eq:mbar_free_energy_difference}
\Delta f_{ij} = - \ln \frac{\sum_{k=1}^m p_k \langle e^{-w_j} \rangle_k}{\sum_{k=1}^m p_k \langle e^{-w_i} \rangle_k}.
\end{equation*}

The delta method:
\begin{align*}
\sigma^2_{\Delta \hat f_{ij}} &= \frac{1}{e^{-\Delta f_{ij}}} \sum_{k=1}^m \Bigg[ \left(\frac{p_k}{\sum_{l=1}^m p_l \langle e^{-w_i} \rangle_l } \right)^2 \sigma^2_{\langle e^{-w_j} \rangle_k} \\
&+ \left( \frac{\sum_{l=1}^m p_l \langle e^{-w_j} \rangle_l}{(\sum_{l=1}^m p_l \langle e^{-w_i} \rangle_l)^2}\right)^2 p_k^2 \sigma^2_{\langle e^{-w_i} \rangle_k} \Bigg]
\end{align*}

\section{Conclusion}

Free-energy perturbation is an accurate method when the typical configurations of the target state form a significant subset of the typical configurations of the sampled state.

We tried to interpret different methods simply as distinct choices of a reduced potential $u_0(x)$ for the reference state, expressed as an analytical function $u_0 = u_0(u_1,\dots,u_n)$.

\bibliography{gmbar}

\end{document}
