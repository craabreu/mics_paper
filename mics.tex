\documentclass[aip,jcp,reprint,amsmath,amssymb]{revtex4-1}
%\documentclass[aip,jcp,preprint,amsmath,amssymb]{revtex4-1}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[pdftex]{graphicx}

\newcommand{\mt}[1]{\boldsymbol{\mathbf{#1}}}           % matrix symbol
\newcommand{\vt}[1]{\boldsymbol{\mathbf{#1}}}           % vector symbol
\newcommand{\tr}[1]{#1^\text{t}}                        % transposition

\begin{document}

\title{Free Energy Computation and Property Reweighting From Multiple Autocorrelated Datasets}

\author{Charlles R. A. Abreu}
\email{abreu@eq.ufrj.br}
\affiliation{Chemical Engineering Department, Escola de Quimica, Universidade Federal do Rio de Janeiro, Rio de Janeiro, RJ 21941-909, Brazil}

\date{\today}

\begin{abstract}
Abstract.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:introduction}

The purpose of this paper is twofold. First, we show that MBAR is intimately related to the expanded ensemble method. Second, and most importantly, 

We agree with Shirts\cite{Shirts_2017} in that the density and mathematical complexity of the original  paper has limited the extent of its appreciation and utilization. For this reason, we decided to present a somewhat lengthy preamble in order to contextualize our proposal and introduce most symbols and needed formulas beforehand, so that the method itself demands a relatively short presentation.

\section{Preamble}

\subsection{Importance Sampling}
\label{sec:definitions}

Let $x$ represent the set of coordinates of a system that can be found at different equilibrium states. Each state $i$ is a statistical ensemble whose probability density is given by
\begin{equation}
\label{eq:state_prob_density}
\rho_i(x) = \frac{1}{Z_i} e^{-u_i(x)},
\end{equation}
where $u_i(x)$ is a \textit{reduced potential}\cite{Shirts_2008, Chodera_2011} with known functional form, while $Z_i = \int e^{-u_i(x)}dx$ is the configurational integral of the system at state $i$. The ensemble average of a property $A(x)$ at state $i$ is defined as
\begin{equation}
\label{eq:ensemble average}
\langle A \rangle_i = \int A(x)\rho_i(x)dx.
\end{equation}

Assume that a sample of configurations $S_i = \{x_{i,k}\}_{k=1}^{n_i}$ was obtained via importance sampling\cite{Allen_1987} (e.g. MC or MD) at state $i$. In this case, we can estimate $\langle A \rangle_i$ by a simple arithmetic average, that is,
\begin{equation}
\label{eq:average estimator}
\overline A_i = \frac{1}{n_i} \sum_{k=1}^{n_i} A(x_{i,k}).
\end{equation}

This estimator naturally extends to multivalued properties, such as vectors and matrices, and is valid whether the configurations in $S_i$ are independent and identically distributed (iid) or form a correlated time series. The difference resides in how to estimate the uncertainty of an average $\overline A_i$ or, more generally, of a function $h(\overline {\vt y}_i)$, where $\vt y(x)$ is a vector of configurational properties $\tr{[A(x) \; B(x) \; \cdots]}$. If we define $\hat{\mt \Sigma}_{\overline{\vt y}_i\overline{\vt y}_i}$ as the matrix of estimated asymptotic covariances between the entries of $\overline{\vt y}_i$, then a squared-error like $\delta^2 \overline A_i$ can be taken as the corresponding main-diagonal entry of $\hat{\mt \Sigma}_{\overline{\vt y}_i\overline{\vt y}_i}$. In the case of $h(\overline{\vt y}_i)$, the squared-error $\delta^2 h$ might be computed by the delta method,\cite{Greene_2012} from which
\begin{equation}
\label{eq:delta method}
\delta^2 h = \tr{(\nabla_{\overline{\vt y}_i} h)} \hat{\mt \Sigma}_{\overline{\vt y}_i\overline{\vt y}_i}(\nabla_{\overline{\vt y}_i} h),
\end{equation} 
where $\nabla$ is the gradient operator. From the central limit theorem (CLT) for an iid sample, the asymptotic cross-covariance matrix between averages $\overline{\vt y}_i$ and $\overline{\vt z}_i$ can be estimated by
\begin{equation*}
\label{eq:asymptotic covariance IID}
\hat{\mt \Sigma}^\ast_{\overline{\vt y}_i\overline{\vt z}_i} = \frac{\sum\limits_{k=1}^{n_i} \left[\vt y(x_{i,k}) - \overline{\vt y}_i\right] \tr{\left[\vt z(x_{i,k}) - \overline{\vt z}_i\right]}}{n_i(n_i - 1)}.
\end{equation*}

This is not true if the sample generated at state $i$ is a correlated time series. Several methods exist for estimating asymptotic variances in this case, such as batching, spectral analysis, and regenerative simulations.\cite{Geyer_1992, Alexopoulos_2006, Flegal_2010, Doss_2014} Extensions to the computation of covariance matrices have also been validated.\cite{Vats_2015, Vats_2017} Spectral methods usually perform better than batching,\cite{Flegal_2010} but they are less simple and more computationally demanding, especially in the multivariate case.\cite{Vats_2015} In this context, the overlapping batch-mean (OBM) estimator\cite{Meketon_1984} is particularly appealing for being asymptotically equivalent to a spectral method while still being very simple to implement. For a sample $S_i$, it consists in defining $n_i-b_i+1$ overlapping blocks of size $b_i$, where each block $j$ is the set of consecutive configurations from $x_{i,j}$ to $x_{i,j+b_i-1}$. The average of $\vt y(x)$ for the first block is ${\overline{\vt y}}^b_{i,1} = \frac{1}{b_i} \sum_{k=1}^{b_i} \vt y(x_{i,k})$ and, for the remaining blocks, they can be computed recursively by
\begin{equation*}
{\overline{\vt y}}^b_{i,j+1} = {\overline{\vt y}}^b_{i,j} + \frac{\vt y(x_{i,j+b_i}) - \vt y(x_{i,j})}{b_i}.
\end{equation*}

Then, the OBM estimator for the asymptotic cross-covariance matrix regarding averages $\overline{\vt y}_i$ and $\overline{\vt z}_i$ is\cite{Meketon_1984}
\begin{equation}
\label{eq:obm asymptotic covariance}
\hat{\mt \Sigma}_{\overline{\vt y}_i\overline{\vt z}_i} = \frac{b_i \sum\limits_{j=1}^{n_i - b_i + 1} ({\overline{\vt y}}^b_{i,j} - \overline{\vt y}_i) \tr{({\overline{\vt z}}^b_{i,j} - \overline{\vt z}_i)}}{(n_i - b_i)(n_i - b_i + 1)}.
\end{equation}

An issue that remains is to determine a proper block size for each sample $S_i$. Asymptotic consistency requires that $b_i$ increases with $n_i$, and it is a common practice to make $b_i = \lfloor n_i^\nu \rfloor$ for some $0 < \nu < 1$, where $\lfloor \cdot \rfloor$ is the floor operator. Such as Flegal and Jones\cite{Flegal_2010} recommend, here we employ $\nu = 1/2$ and, therefore, $b_i = \lfloor \sqrt{n_i} \rfloor$.

\subsection{Free-Energy Computation and Property Reweighting}

We are often interested in the difference between the reduced free energies of two states, defined as $\Delta f_{ij} = f_j - f_i = - \ln (Z_j/Z_i)$. As it is usually unfeasible to compute absolute free energies, we find appropriate to define $f_i$ as a relative value with respect to a state~$0$, chosen as a common reference to all other states of interest, so that $f_i = -\ln (Z_i/Z_0)$. We can thus rewrite Eq.~\eqref{eq:state_prob_density} as
\begin{equation}
\label{eq:state_prob_density_Z0}
\rho_i(x) = \frac{1}{Z_0} e^{-u_i(x)+ f_i}.
\end{equation}

The value of $f_i$ can only be determined up to an additive constant and this fact will be assumed henceforth. One can estimate $f_i$ by relating it to one or more ensemble averages. We must remark, however, that importance sampling usually overlooks the tails of $\rho_i(x)$, so that only a limited region $C_i$ of the configurational space is sampled with reasonable accuracy. Following Jarzynski,\cite{Jarzynski_2006} we refer to $C_i$ as the set of \textit{typical} configurations of state $i$. It does not necessarily coincide with the set $D_i$ of configurations that substantially contribute to the integral in Eq.~\eqref{eq:ensemble average}, referred to as the \textit{dominant} configurations. In fact, the estimator in Eq.~\eqref{eq:average estimator} will be accurate only if $D_i$ is a significant subset of $C_i$. This issue is particularly important in the calculation of free-energy differences and their uncertainties.

By using methods such as Free Energy Perturbation (FEP),\cite{Zwanzig_1954} Simple Overlap Sampling,\cite{Lee_1980, Lu_2003} and Bennett Acceptance Ratio (BAR),\cite{Bennett_1976} free-energy differences can be computed for those pairs of states whose sets of typical configurations overlap most considerably and then pieced together to provide differences between poorly- or non-overlapping states. This strategy is known as \textit{staging}.\cite{Kofke_1998} Among the cited methods, FEP is the least recommended one. Given two states $i$ and $j$, it consists in evaluating either $\Delta f_{ij} = -\ln \langle e^{u_i - u_j} \rangle_i$ or $\Delta f_{ij} = \ln \langle e^{u_j - u_i} \rangle_j$.\cite{Zwanzig_1954} It happens, however, that these averages entail dominant sets $D_i$ and $D_j$ that practically coincide with the typical sets $C_j$ and $C_i$, respectively, which is opposite to the situation that would maximize accuracy.\cite{Jarzynski_2006} On the other hand, BAR is the most accurate method because it manages, in an iterative way, to relate $\Delta f_{ij}$ to ensemble averages computed at states $i$ and $j$ whose dominant sets $D_i$ and $D_j$ fulfill the intersection between $C_i$ and $C_j$.

Another way of tackling overlap deficiency consists in defining a reference state whose set $C_0$ is a contiguous region that encompasses all sets $C_i$ associated to a number of states of interest. Then, instead of sampling configurations at these states, we do it at state $0$, with probability density $\rho_0(x) = \frac{1}{Z_0} e^{-u_0(x)}$, and compute every $f_i$ by means of the perturbation formula
\begin{equation}
\label{eq:nbs sampling free energy}
f_i = -\ln \langle e^{u_0-u_i} \rangle_0.
\end{equation}

As explained above, the dominant set $D_0$ will coincide with $C_i$, thus being a subset of $C_0$ as desired. Torrie and Valleau\cite{Torrie_1977} pioneered this idea, known as \textit{non-Boltzmann sampling} due to the usually non-physical character of the sampled state. One is also able to compute, directly from the configurations sampled at state $0$, ensemble averages of any property $A$ at the states of interest. This procedure, known as \textit{reweighting}, is based on the identity\cite{Torrie_1977}
\begin{equation}
\label{eq:nbs sampling reweighting}
\langle A \rangle_i = \frac{\langle A e^{u_0 - u_i} \rangle_0}{\langle e^{u_0 - u_i} \rangle_0} % = \langle A e^{u_0 - u_i + f_i} \rangle_0.
\end{equation}

The multicanonical approach\cite{Berg_1992, Lee_1993, Abreu_2006} and the enveloping distribution method\cite{Christ_2007, *Christ_2008, *Christ_2009} are examples of successful non-Boltzmann sampling techniques.

\subsection{Expanded Ensemble}
\label{sec:expanded ensemble}
	
A method of particular relevance in the train of thought we develop here is the one known as \textit{expanded ensemble}.\cite{Lyubartsev_1992} It is a non-Boltzmann sampling scheme, but closely related to staging as it consists in combining a number $m$ of states whose distributions overlap. However, instead of sampling each state independently, we perform importance sampling from a joint probability of configurations and states given by\cite{Nymeyer_2010}
\begin{equation}
\label{eq:expanded ensemble joint}
p_0(x, i) = \frac{1}{Z_0} e^{-u_i(x) + \eta_i},
\end{equation}
where $\vt \eta$ is a vector of arbitrary weighting factors. This results in a sample $S = \{x_k,i_k\}_{k=1}^n$, where each configuration bears a state-indicating label. For completeness, we describe some ways of performing such sampling in Appendix \ref{sec:expanded ensemble simulation}. From Eq.~\eqref{eq:expanded ensemble joint}, one can derive the marginal probability of each state $i$, defined as $\pi_i = \int p_0(x,i)dx$, as well as the marginal probability density at state~$0$, defined as $\rho_0(x) = \sum_{i=1}^m p_0(x,i)$. One can also obtain the conditional probabilities $p(i|x)$ and $p(x|i)$ via Bayes' theorem, $p_0(x,i) = p(x|i) \pi_i = p(i|x) \rho_0(x)$. Hence,
\begin{subequations}
\label{eq:expanded ensemble probabilities}
\begin{gather}
\pi_i = \frac{Z_i}{Z_0} e^{\eta_i} = e^{\eta_i - f_i}, \label{eq:expanded ensemble prior} \\
\rho_0(x) = \frac{1}{Z_0} \sum_{j=1}^m e^{-u_j(x) + \eta_j}, \label{eq:expanded ensemble evidence} \\
p(i|x) = \frac{e^{-u_i(x) + \eta_i}}{\sum_{j=1}^m e^{-u_j(x) + \eta_j}}, \; \text{and} \label{eq:expanded ensemble posterior} \\
p(x|i) = \frac{1}{Z_0} e^{u_i(x) + f_i}. \label{eq:expanded ensemble likelihood}
\end{gather}
\end{subequations}

The fact that $p(x|i) = \rho_i(x)$ demonstrates that each individual state is sampled correctly. Once the sample $S$ has been generated, a simple estimator for the relative free energy of each state stems from Eq.~\eqref{eq:expanded ensemble prior} by making $\hat \pi_i = \frac{n_i}{n}$, where $n_i$ is the number of appearances of state $i$ in $S$. The result is
\begin{equation}
\label{eq:expanded ensemble histogram estimator}
\hat f_i = \eta_i - \ln \frac{n_i}{n}.
\end{equation}

However, we can derive a much better estimator\cite{Ding_2017} once we realize that $\pi_i = \int \rho_0(x) p(i|x) dx$, meaning that $\pi_i$ is the ensemble average of the marginal probability $p(i|x)$ at state $0$. The new estimator, which arises if we simply use Eq.~\eqref{eq:average estimator} to estimate such an average, is
\begin{equation}
\label{eq:expanded ensemble FEP estimator}
\hat f_i = -\ln \left[ \frac{1}{n}\sum_{k=1}^n \frac{e^{-u_i(x_k)}}{\sum_{j=1}^m e^{-u_j(x_k) + \eta_j}} \right].
\end{equation}

Note that the state-indicating labels in $S$ are neglected in both estimators, due to an essential feature of importance sampling in that every sampled configuration contributes equally to an ensemble average. In practice, the set of weighting factors is often chosen so that all states are visited with equal frequency. This requires that $\eta_i = f_i - \ln m$, which can only be achieved iteratively. Other strategies have also been proposed to select suitable weighting factors or reduced potentials that improve performance and avoid ergodicity issues. Details can be found in Refs.~\onlinecite{Katzgraber_2006, *Trebst_2006, Escobedo_2007, *Escobedo_2008, *Martinez_2008}.

Finally, we can combine the results in Eq.~\eqref{eq:expanded ensemble probabilities} in order to show that an expanded ensemble distribution is actually a \textit{mixture model}\cite{Lindsay_1995, Marin_2005} composed of the distributions corresponding to the $m$ states, that is,
\begin{equation}
\label{eq:mixture ensemble}
\rho_0(x) = \sum_{i=1}^m \pi_i \rho_i(x).
\end{equation}

In this context, the vector $\vt \pi$ of state probabilities can be regarded as the mixture composition. Moreover, since $\rho_0 = \frac{1}{Z_0}e^{-u_0}$, we can resort to Eq.~\eqref{eq:state_prob_density_Z0} for expressing the reduced potential of the mixture state as
\begin{equation}
\label{eq:mixture potential}
u_0(x) = -\ln \sum_{i=1}^m \pi_i e^{-u_i(x) + f_i}.
\end{equation}

With $\vt f$ replaced by its estimator $\hat{\vt f}$, this potential can be employed with Eqs.~\eqref{eq:nbs sampling free energy} and \eqref{eq:nbs sampling reweighting} to estimate relative free energies and other ensemble averages at any state whose set $C$ is a subset of $C_0$, whether such state is part of the expanded ensemble or not. Following Geyer\cite{Geyer_1994} and Shirts,\cite{Shirts_2017} we refer to this procedure as ``reweighting from the mixture''.

\subsection{Multistate Bennett Acceptance Ratio}

An interesting feature of an expanded ensemble is that the walk throughout states favors ergodicity, as compared to sampling each state individually. Nonetheless, the process of determining convenient states and weights can be tedious and time-consuming. It is thus more likely that we face the situation described in Sec.~\ref{sec:definitions}, when samples individually drawn at different states are available. An advantage of this scenario is that inclusion of new states, if necessary, is straightforward. In this case, the pooled sample can be expressed as $\mathcal S = \big\{\{x_{i,k}\}_{k=1}^{n_i} \big\}_{i=1}^m$. Shirts and Chodera\cite{Shirts_2008} described a method for computing relative free energies and their uncertainties by using all configurations in $\mathcal S$, provided that each sample is composed of independent and identically distributed (iid) configurations. The authors named the method as Multistate Bennet Acceptance Ratio (MBAR) because it is equivalent to BAR when only two states are involved. Let us derive their estimator from a simple argument before embarking in a detailed analysis. MBAR consists in assigning the same importance to every configuration, regardless of which state it comes from. This makes $\mathcal S$ equivalent to the outcome of an expanded ensemble simulation, so that we can overlook its subset structure and write $\mathcal S = \{x_k\}_{k=1}^n$. The issue of not knowing the weighting factors of this expanded ensemble is easily tackled by combining the two estimators of Eqs.~\eqref{eq:expanded ensemble histogram estimator} and \eqref{eq:expanded ensemble FEP estimator}. Elimination of $\vt \eta$ yields
\begin{equation}
\label{eq:mbar free energy estimator}
\hat f_i = -\ln \sum_{k=1}^n \frac{e^{-u_i(x_k)}}{\sum_{j=1}^m n_j e^{-u_j(x_k) + \hat f_j}},
\end{equation}
which is equivalent to Eq.~11 of Ref.~\onlinecite{Shirts_2008}. This is not an isolated estimator, but a self-consistent system of equations whose solution provides the whole vector $\hat {\vt f}$. In fact, because the solution can only be determined up to an additive constant, one can fix $\hat f_1$ and solve the remaining equations in order to obtain $\hat{\vt f}_{2:m}$. After that, we can use $\hat{\vt f}$ to perform FEP and reweighting calculations, exactly as described in the preceding section.

As the $m$ samples in $\mathcal S$ are generated independently, giving all configurations the same importance seems unsuitable if these samples exhibit correlation and distinct mixing times. This is one reason why MBAR is meant to deal with iid samples and requires subsampling whenever some sample is originally correlated.\cite{Shirts_2008} Another important reason is concerned with the determination of uncertainties. The MBAR estimator had previously appeared in the literature under other denominations such as Biased Sampling,\cite{Vardi_1985, *Gill_1988} Reverse Logistic Regression,\cite{Geyer_1994} and Extended Bridge Sampling.\cite{Meng_1996, Kong_2003, Tan_2004} It has been derived in several ways from the principle of maximum likelihood, which can lead to standardized proofs of CLT's, thus providing reliable estimates of asymptotic covariance matrices.\cite{Pawitan_2001, Greene_2012} These proofs, however, are usually based on the premise that iid samples are available. More details on subsampling and uncertainty estimation are given in Appendix~\ref{sec:subsampling and uncertainty in MBAR}.

Finally, we remark that our interpretation of MBAR as an expanded ensemble with unknown weights unifies two recent contributions: a revised presentation of the method by Shirts\cite{Shirts_2017} (based on the work of Geyer\cite{Geyer_1994}) and an alternative derivation by Ding and coworkers.\cite{Ding_2017}

\section{Taking Full Advantage of Correlated Datasets from Multiple States}

The requirement of subsampling often causes MBAR to waste a considerable amount of data. Even though the discarded configurations carry less information than do the ones that remain, they could possibly have a positive impact on the quality of a statistical analysis. In what follows, we develop an unwasteful extension of MBAR, which allows one to use all available data even if individual samples exhibit distinct mixing times.

\subsection{Relative Free Energies of Sampled States}

The starting point of our proposal is an exact expression obtained from Eqs.~\eqref{eq:nbs sampling free energy} and \eqref{eq:mixture potential}, which is
\begin{equation}
\label{eq:free energy exact}
f_i = -\ln \left\langle \frac{e^{-u_i}}{\sum_{j=1}^m \pi_j e^{-u_j + f_j}} \right\rangle_0.
\end{equation}

In MBAR, the average above is estimated by applying Eq.~\eqref{eq:average estimator} to the overall sample $\mathcal S$. This decision causes all the restraints discussed in the preceding section because all configurations in $\mathcal S$ must be identically distributed according to $\rho_0(x)$ so that both $\vt \pi$ and $\vt f$ can be properly determined. Nevertheless, one can show via Eqs.~\eqref{eq:ensemble average} and \eqref{eq:mixture ensemble} that, for a mixture model, $\langle A \rangle_0 = \sum_{i=1}^m \pi_i \langle A \rangle_i$. Therefore, one can compute an average at the mixture state from averages at the individual ones, that is,
\begin{equation}
\label{eq:mixture average estimator}
{\overline A}_0 = \sum_{i=1}^m \pi_i \overline{A}_i = \sum_{i=1}^m \frac{\pi_i}{n_i} \sum_{k=1}^m A(x_{i,k}).
\end{equation}

We will refer to this estimator as a \textit{mixture of independently collected samples} (MICS). It requires no assumption on the distribution of the pooled sample as a whole. Moreover, it is valid for any mixture composition $\vt \pi$, which can then be regarded as a set of prior probabilities (i.e. specified beforehand). This makes it easy to combine OBM-based covariance matrices like $\hat{\mt \Sigma}_{\overline{\vt y}_i \overline{\vt z}_i}$, obtained via Eq.~\eqref{eq:obm asymptotic covariance} at each state $i$, into a matrix of covariances between averages computed at the mixture state. Because $\overline{\vt y}_0 = \sum_{i=1}^m \pi_i \overline{\vt y}_i$ and each average $\overline{\vt y}_i$ is independent, the fact that each $\pi_i$ is a known constant results in
\begin{equation}
\label{eq:mixture variance estimator}
\hat{\mt \Sigma}_{\overline{\vt y}_0 \overline{\vt z}_0} = \sum_{i=1}^m \pi_i^2 \hat{\mt \Sigma}_{\overline{\vt y}_i \overline{\vt z}_i}.
\end{equation}

Therefore, the choice of $\vt \pi$ is expected to affect the asymptotic behavior of the MICS estimator, which will be a topic of discussion shortly. Finally, the equation we propose for estimating the relative free energies of the sampled states, derived from applying the MICS estimator to Eq.~\eqref{eq:mixture average estimator}, is
\begin{equation}
\label{eq:mblock free energy estimator}
{\hat f}_i = -\ln \sum_{j=1}^m \frac{\pi_j}{n_j} \sum_{k=1}^{n_j} \frac{e^{-u_i(x_{j,k})}}{\sum_{s=1}^m \pi_s e^{-u_s(x_{j,k}) + {\hat f}_s}}.
\end{equation}

As with MBAR, we again have a system of equations that must be solved self-consistently after setting the value of $\hat f_1$, for instance. In contrast, however, the state at which each configuration was sampled is now a relevant information, unless we specify $\pi_j = {n_j}/{n}$ in order to recover Eq.~\eqref{eq:mbar free energy estimator}. Extending an approach pioneered by Geyer,\cite{Geyer_1994} Doss and Tan\cite{Doss_2014} proposed an equivalent estimator and investigated its asymptotic behavior when covariances at individual states are computed via regenerative simulations. In a subsequent study, Roy \textit{et al}.\cite{Roy_2018} extended their findings to the case of covariances based on non-overlapping batch-means. They were also able to soften the conditions required for proving the consistency of the estimator. Here we assume the observance of these conditions and adopt the OBM strategy described in Sec.~\ref{sec:definitions}.

As Roy \textit{et al}.\cite{Roy_2018} suggested, an adequate specification for the marginal probabilities $\vt \pi$ should include some definition of effective sample size.


should instead incorporate the effective sample size of different 

 and our results will confirm, an adequate specification for the marginal probability of each state $i$ is
\begin{equation}
\label{eq:mblock prior}
\pi_i = \frac{n^\text{eff}_i}{\sum_{j=1}^m n^\text{eff}_j},
\end{equation}
where $n^\text{eff}_i$ is the effective size of the sample obtained at state $i$.

, defined as the number of iid configurations 

%\begin{equation*}
%n^\text{eff}_i = n_i \bigg( \frac{\det \hat{\mt \Sigma}^\text{iid}_{\overline{\vt y}_i \overline{\vt y}_i} }{\det \hat{\mt \Sigma}_{\overline{\vt y}_i \overline{\vt y}_i} } \bigg)^{1/p}.
%\end{equation*}
\begin{equation*}
n^\text{eff}_i = n_i \frac{\hat{\Sigma}_{\bar u_i \bar u_i}}{\hat{\Sigma}^\ast_{\bar u_i \bar u_i}}
\end{equation*}

The assumption of uncorrelated blocks will be crucial for determining the asymptotic uncertainty of the free-energy estimator in Eq.~\eqref{eq:mblock free energy estimator}.

\subsection{Maximum Likelihood Approach and Numerical Computation}

Following Doss and Tan,\cite{Doss_2014} we can derive the non-linear system in Eq.~\eqref{eq:mblock free energy estimator} by a maximum likelihood approach, based on the log-quasi-likelihood function\cite{Doss_2014, Tan_2015, Roy_2018}
\begin{equation}
\label{eq:mblock log-quasi-likelihood}
\ln \mathcal L = \sum_{i=1}^m \frac{\pi_i}{n_i} \sum_{k=1}^{n_i} \ln p_i(x_{i,k}),
\end{equation}
where $p_i(x)$ is the conditional probability $p(i|x)$, obtained by combining Eqs.~\eqref{eq:expanded ensemble prior} and \eqref{eq:expanded ensemble posterior}, that is,
\begin{equation}
\label{eq:mixture posterior probability}
p_i(x) = \frac{\pi_i e^{-u_i(x) + f_i}}{\sum_{j=1}^m \pi_j e^{-u_j(x) + f_j}}.
\end{equation}

The first- and second-order derivatives of $\ln p_k(x)$ with respect to $f_i$ and $f_j$ are, respectively,
\begin{equation*}
\frac{\partial \ln p_k}{\partial f_i} = \delta_{i,k} - p_i \quad \text{and} \quad \frac{\partial^2\ln p_k}{\partial f_i \partial f_j} = -p_i(\delta_{i,j} - p_j),
\end{equation*}
where $\delta_{i,j}$ is the Kronecker delta. It is now convenient to introduce a vector/matrix notation. First, we define a vector-valued configurational property $\vt p(x)$, whose each entry $i$ is $p_i(x)$, given by Eq.~\eqref{eq:mixture posterior probability}. Next, a matrix-valued property $\mt B(x)$ is defined, where $\mt B = \text{diag}(\vt p) - {\vt p}\tr{\vt p}$. We remark that $\mt B$ is both symmetric and singular, as it satisfies $\tr{\mt B} = \mt B$ and $\mt B\vt 1 = \vt 0$, where $\vt 1$ is a vector of ones and $\vt 0$ is a vector of zeros. Note that the MICS estimator naturally extends to multivalued properties, so that we can use Eq.~\eqref{eq:mixture average estimator} to compute averages like $\overline{\vt p}_0$ and $\overline{\mt B}_0$. Then, the reciprocal gradient and reciprocal Hessian of the log-quasi-likelihood function with respect to the relative free-energy vector $\vt f$ are, respectively,
\begin{equation}
\label{eq:mblock score vector}
\vt g = -\nabla_{\vt f} \ln \mathcal L = \overline{\vt p}_0 - \vt \pi
\end{equation}
and
\begin{equation}
\label{eq:mblock fisher information matrix}
\mt H = -\nabla^2_{\vt f} \ln \mathcal L = \overline{\mt B}_0.
\end{equation}

For a given pooled sample $\mathcal S$ and predefined mixture composition $\vt \pi$, both $\vt g$ and $\mt H$ are exclusive functions of $\vt f$. Thus, the maximum likelihood estimator is the solution of $\vt g(\hat{\vt f}) = \vt 0$, which occurs when $\overline{\vt p}_0 = \vt \pi$, that is, when the computed expectation of $\vt p(x)$ at the mixture state equals the specified mixture composition. This is just a reformulation of Eq.~\eqref{eq:mblock free energy estimator}, provided that $\overline{\vt p}_0$ is defined as in Eq.~\eqref{eq:mixture average estimator}. Another interesting interpretation is possible by defining a stochastic matrix $\mt O = [\begin{array}{ccc} \overline{\vt p}_1 & \cdots & \overline{\vt p}_m \end{array}]$, where each $\overline{\vt p}_i$ is computed via Eq.~\eqref{eq:average estimator}. This is the \textit{overlap matrix} which Klimovich \textit{et al}.\cite{Klimovich_2015} defined as a tool for quantifying phase-space overlap in multistate methods. Such definition allows us to write $\mt O \vt \pi = \vt \pi$ as yet another form for Eq.~\eqref{eq:mixture average estimator}, meaning that the estimator $\hat{\vt f}$ is the one that causes the overlap matrix to have a stationary distribution that coincides with the mixture composition.

Application of Eq.~\eqref{eq:mixture average estimator} to compute both $\vt g$ and $\mt H$ requires the vector of relative free energies. One can find an initial guess by doing direct calculations (e.g. via overlap sampling\cite{Lee_1980, Lu_2003}) for pairs of adjacent states with respect to the averages of their own reduced potentials (i.e. $\langle u_i \rangle_i$). The solution of Eq.~\eqref{eq:mblock score vector} can be found by enforcing $\hat f_1 = 0$ and then updating $\hat{\vt f}_{2:m}$ up to convergence via Newton-Raphson iterations\cite{Shirts_2008}
\begin{equation*}
\label{eq:mblock Newton-Raphson}
\hat{\vt f}_{2:m} \leftarrow \hat{\vt f}_{2:m} - {\mt H}_{2:m,2:m}(\hat{\vt f}) \backslash {\vt g}_{2:m}(\hat{\vt f}),
\end{equation*}
where $\mt A \backslash \vt b$ is the solution of linear system $\mt A \vt x = \vt b$. The log-quasi-likelihood function in Eq.~\eqref{eq:mblock log-quasi-likelihood} is concave,\cite{Doss_2014} which makes the solution above unique. In addition, according to a theorem demonstrated in Ref.~\onlinecite{Doss_2014}, $\hat{\vt f}$ is a consistent estimator, meaning that it converges to the true vector $\vt f$ when $n \rightarrow \infty$ with fixed ratios $n_i/n$ for all $i$.

\subsection{Uncertainties of Free-Energy Differences}

The free-energy difference between two sampled states $i$ and $j$ can be estimated by $\Delta \hat f_{i,j} = \hat f_j - \hat f_i$. The square-error 

Hence, we can deduce from the delta method, Eq.~\eqref{eq:delta method}, that
\begin{equation}
\delta^2 \Delta f_{i,j} = \hat \Theta_{i,i} + \hat \Theta_{j,j} - 2\hat \Theta_{i,j},
\end{equation}
where $\hat \Theta_{i,j}$ is the estimated asymptotic covariance between $\hat f_i$ and $\hat f_j$. In accordance with proofs provided in Refs.~\onlinecite{Geyer_1994} and \onlinecite{Doss_2014}, a formula that can be used to compute the complete asymptotic covariance matrix $\hat{\mt \Theta}$ is
\begin{equation}
\label{eq:mblock covariance matrix}
\hat{\mt \Theta} = \overline{\mt B}_0^\dag \hat{\mt \Sigma}_{\overline{\vt p}_0 \overline{\vt p}_0} \overline{\mt B}_0^\dag,
\end{equation}
where $\hat{\mt \Sigma}_{\overline{\vt p}_0 \overline{\vt p}_0}$ is matrix of covariances between the averages of $\{p_i(x)\}_{i=1}^m$ at the mixture state. A simple rationale for this result stems from the delta method. It is clear from Eqs.~\eqref{eq:mblock score vector} and \eqref{eq:mblock fisher information matrix} that $\nabla_{\vt f}\overline{\vt p}_0 = \overline{\mt B}_0$. Because this matrix is symmetric, it follows from Eq.~\eqref{eq:delta method} that $\hat{\mt \Sigma}_{\overline{\vt p}_0 \overline{\vt p}_0} = \overline{\mt B}_0 \hat{\mt \Theta} \overline{\mt B}_0$. Thus, Eq.~\eqref{eq:mblock covariance matrix} is nothing but a solution of this matrix equation, which requires a Moore-Penrose pseudoinverse because $\overline{\mt B}_0 \vt 1 = \vt 0$.

By making $\overline{\vt y}_0 = \overline{\vt z}_0 = \overline{\vt p}_0$, this equation can be used together with Eq.~\eqref{eq:obm asymptotic covariance} to evaluate $\hat{\mt \Theta}$ via Eq.~\eqref{eq:mblock covariance matrix}.

\subsection{Free-Energy Perturbation and Reweighting}

Considering that $\langle{\vt y}\rangle_0$ is the average of $\vt y(x)$ at state $0$ computed with a vector $\vt f$ close to $\hat{\vt f}$
\begin{equation*}
\overline{\vt y}^{\vt f}_0 \approx \overline{\vt y}_0 + \tr{(\nabla_{\vt f}\overline{\vt y}_0)}({\vt f} - \hat{\vt f}),
\end{equation*}
where $\nabla_{\vt f}\overline{\vt y}_0$ is evaluated at $\hat{\vt f}$.

\begin{equation}
\text{var}(\hat{\vt y}_0) = \text{var}(\overline{\vt y}_0) + 2\tr{(\nabla_{\vt f}\overline{\vt y}_0)}\text{cov}(\hat{\vt f},\overline{\vt y}_0) + \tr{(\nabla_{\vt f}\overline{\vt y}_0)}\text{var}(\hat{\vt f})(\nabla_{\vt f}\overline{\vt y}_0)
\end{equation}

Bilinearity:
\begin{gather*}
\text{Cov}(\mt A \mt x + \mt y + \vt b,\mt A \mt x + \mt y + \vt b) = \\ \text{Cov}(\mt A \mt x,\mt A \mt x + \mt y) + \text{Cov}(\mt y,\mt A \mt x + \mt y) = \\ \text{Cov}(\mt A \mt x,\mt A \mt x) + 2 \text{Cov}(\mt A \mt x,\mt y) + \text{Cov}(\mt y,\mt y) = \\ \mt A \text{Cov}(\mt x,\mt x) \tr{\mt A} + 2 \mt A \text{Cov}(\mt x,\mt y) + \text{Cov}(\mt y,\mt y)
\end{gather*}

\begin{equation}
\hat{\mt \Xi} = \hat{\mt \sigma}_{\vt y\vt y} + 2\tr{(\nabla_{\vt f}\overline{\vt y}_0)} \overline{\mt B}_0^\dag \hat{\mt \sigma}_{\vt p\vt y} + \tr{(\nabla_{\vt f}\overline{\vt y}_0)}\hat{\mt \Theta}{(\nabla_{\vt f}\overline{\vt y}_0)}
\end{equation}

Defining a new matrix-valued configurational property $\mt Z(x)$ so that $\mt Z = \nabla_{\vt f}\vt y$:
\begin{equation}
\hat{\mt \Xi} = \hat{\mt \sigma}_{\vt y\vt y} + 2\tr{\overline{\mt Z}_0} \overline{\mt B}_0^\dag \hat{\mt \sigma}_{\vt p\vt y} + \tr{\overline{\mt Z}_0}\hat{\mt \Theta}{\overline{\mt Z}_0}
\end{equation}

\begin{equation}
{\vt y}(x) = e^{u_0(x)} \left[\begin{array}{c} A(x)e^{-u(x)} \\ e^{-u(x)} \end{array}\right]
\end{equation}

\begin{equation}
\frac{\partial \vt y}{\partial f_i} = -\frac{\pi_i e^{-u_i(x) + \hat f_i}}{\left(\sum_{i=1}^m \pi_j e^{-u_j(x) + \hat f_j}\right)^2} \left[\begin{array}{c} A(x)e^{-u(x)} \\ e^{-u(x)} \end{array}\right]
\end{equation}

\begin{equation}
\mt Z(x) = -{\vt y(x)}\tr{\vt p}(x)
\end{equation}

\subsection{Derivatives}

Given $u(x,\lambda)$, calculate $\frac{\partial f}{\partial \lambda}$ and $\frac{\partial \langle A \rangle}{\partial \lambda}$.

\section{Results}

\subsection{Propane Solvation in Water}

In order to compare the proposed method against MBAR in regard to the calculation of relative free energies of sampled states, we simulated the infinite-dilution solvation of propane in water. Both molecules are modeled as rigid bodies. The intermolecular interactions between water atoms and propane pseudo-atoms are calculated by the softcore potential\cite{Beutler_1994} $\nu(r,\lambda) = 4\lambda\epsilon(s^{-2} - s^{-1})$, where $s = (r/\sigma)^6 + 0.5 (1-\lambda)$.

\begin{table}
\caption{Reduced relative free energies}
\label{table:propane solvation}
\begin{ruledtabular}
\begin{tabular}{ccccc}
$\lambda$ & $g$ & MBAR & GMBAR & Optimized \\
\hline
$0.0$ & $12$ & $0.0$ & $0.0$ & $0.0$ \\
$0.1$ & $14$ & $0.4547 \pm 0.0025$ & $0.4563 \pm 0.0018$ \\
$0.2$ & $21$ & $1.7643 \pm 0.0073$ & $1.7668 \pm 0.0056$ \\
$0.3$ & $27$ &  $3.497 \pm 0.018$  &  $3.491 \pm 0.014$ \\
$0.4$ & $12$ &  $4.475 \pm 0.027$  &  $4.462 \pm 0.020$ \\
$0.5$ & $8$  &  $4.728 \pm 0.030$  &  $4.712 \pm 0.022$ \\
$0.6$ & $5$  &  $4.636 \pm 0.031$  &  $4.619 \pm 0.023$ \\
$0.8$ & $4$  &  $3.979 \pm 0.032$  &  $3.958 \pm 0.023$ \\
$1.0$ & $2$  &  $2.989 \pm 0.032$  &  $2.968 \pm 0.024$
\end{tabular}
\end{ruledtabular}
\end{table}

\begin{figure}
\centering
\includegraphics{Figures/nelder_mead}
\caption{}
\label{fig:nelder_mead}
\end{figure}


\section{Conclusion}

Free-energy perturbation is an accurate method when the typical configurations of the target state form a significant subset of the typical configurations of the sampled state.

We tried to interpret different methods simply as distinct choices of a reduced potential $u_0(x)$ for the reference state, expressed as an analytical function $u_0 = u_0(u_1,\dots,u_n)$.

\appendix

\section{Simulation of an Expanded Ensemble}
\label{sec:expanded ensemble simulation}

Rao-Blackwell estimator\cite{Ding_2017}....

One can use it, for instance, in a MC simulation that involves both configurational and state-changing moves.\cite{Lyubartsev_1992} This will result in a sample $S = \{x_k,i_k\}_{k=1}^n$, with each configuration bearing a state-indicating label. Another way is via MC or MD of a system whose reduced potential is computed by
\begin{equation}
%\label{eq:expanded ensemble potential}
u_0(x) = - \ln \sum_{j=1}^m e^{-u_j(x) + \eta_j},
\end{equation}
which results in the probability density of Eq.~\eqref{eq:expanded ensemble evidence}. In this case, the sampled configurations will not bear any labels at first. However, if somehow these labels are needed, we can randomly pick a label for each configuration from the conditional distribution $p(i|x)$.\cite{Nymeyer_2010} From Bayes' theorem $p(x,i) = p(x|i) p(i) = p(i|x) p(x)$, we can divide Eq.~\eqref{eq:expanded ensemble joint} by Eq.~\eqref{eq:expanded ensemble evidence} to find out that
\begin{equation}
%\label{eq:expanded ensemble posterior}
p(i|x) = \frac{e^{-u_i(x) + \eta_i}}{\sum_{j=1}^m e^{-u_j(x) + \eta_j}}.
\end{equation}

In addition, we can also divide Eq.~\eqref{eq:expanded ensemble joint} by Eq.~\eqref{eq:expanded ensemble prior} to see how configurations are distributed within each state. The results is $p(x|i) = Z_0^{-1} e^{u_i(x) + f_i}$, which is identical to Eq.~\eqref{eq:state_prob_density_Z0} and proves that each state of the expanded ensemble is sampled correctly. Together with Bayes' theorem, this can also be used to show that
\begin{equation}
%\label{eq:mixture ensemble}
\rho_0(x) = \sum_{i=1}^m \pi_i \rho_i(x),
\end{equation}
which means that an expanded ensemble distribution is actually a \textit{mixture model}\cite{Lindsay_1995, Marin_2005} composed of the distributions at all states. The method proposed by Nymeyer\cite{Nymeyer_2010} and a special case of the one devised by Christ and van Gunsteren\cite{Christ_2007, *Christ_2008, *Christ_2009} are examples of using Eq.~\eqref{eq:expanded ensemble potential} to perform non-Boltzmann sampling with MD. A third method for sampling from an expanded ensemble is known as \textit{Gibbs sampling}.\cite{Marin_2005, Chodera_2011} While the state $i$ is kept fixed, configurations can be sampled from $p(x|i)$ via MC or MD. Also, while keeping the configuration $x$ fixed, a state can be chosen according to $p(i|x)$. Alternating these procedures will produce the correct expanded ensemble distribution.\cite{Chodera_2011}

Once a sample $S$ has been generated, a simple estimator for the relative free energy of each state stems from Eq.~\eqref{eq:expanded ensemble prior} by making $\hat \pi_i = n_i/n$, where $n_i$ is the number of appearances of state $i$ in $S$. The result is
\begin{equation}
%\label{eq:expanded ensemble histogram estimator}
\hat f_i = \eta_i - \ln \frac{n_i}{n}.
\end{equation}

However, a better estimator can be obtained by combining Eqs.~\eqref{eq:nbs sampling free energy} and \eqref{eq:expanded ensemble potential} and then using Eq.~\eqref{eq:average estimator} to compute the required ensemble average. It is
\begin{equation}
%\label{eq:expanded ensemble FEP estimator}
\hat f_i = -\ln \left[ \frac{1}{n}\sum_{k=1}^n \frac{e^{-u_i(x_k)}}{\sum_{j=1}^m e^{-u_j(x_k) + \eta_j}} \right].
\end{equation}

Both estimators neglect the detailed association of configurations to states. This is due to an essential feature of importance sampling, in that every sampled configuration contributes equally to an ensemble average. During the simulation of a complex system, it is a common practice to store only a small set of collective properties for posterior analysis. In this case, if the second estimator is to be used, one must ensure that such set includes all the potentials present in Eq.~\eqref{eq:expanded ensemble FEP estimator} or other properties from which they can be computed afterwards. Finally, we remark that the set of weighting factors is often chosen so that all states are visited with equal frequency. For this to occur, we must impose that $\eta_i = f_i - \ln m$, which can only be achieved iteratively. Other strategies have also been proposed to select suitable weighting factors or reduced potentials that improve performance and avoid ergodicity issues. Details can be found in Refs.~\onlinecite{Katzgraber_2006, *Trebst_2006, Escobedo_2007, *Escobedo_2008, *Martinez_2008}.

\section{More about MBAR}
\label{sec:subsampling and uncertainty in MBAR}

For a series $\{x_{i,k}\}_{k=1}^{n_i}$ drawn at state $i$, the subsampling interval is set as the statistical inefficiency of some configurational property $A(x)$, which is computed as\cite{Chodera_2007}
\begin{equation}
\label{eq:statistical inefficiency}
g_i = 1 + 2 \sum\limits_{t=1}^{n_i-1} \frac{n_i - t}{n_i} \gamma_i(t),
\end{equation}
where $\gamma_i(t)$ is the normalized autocorrelation function of $A$, calculated by
\begin{equation*}
\gamma_i(t) = \frac{\dfrac{1}{n_i - t} \sum\limits_{k=1}^{n_i-t} \left[A(x_{i,k}) - \overline A_i\right]\left[A(x_{i,k+t}) - \overline A_i\right]}{\dfrac{1}{n_i} \sum\limits_{k=1}^{n_i} \left[A(x_{i,k}) - \overline A_i\right]^2}.
\end{equation*}

The autocorrelation function $\gamma_i(t)$ is expected to asymptotically decay to zero for most properties, but in practice its tail fluctuates around zero. Chodera \textit{et al}.\cite{Chodera_2007} proposed that the sum in Eq.~\eqref{eq:statistical inefficiency} should be truncated right before $\gamma_i(t)$ crosses zero for the first time, arguing that the function becomes statistically indistinguishable from zero at this point.

Shirts and Chodera\cite{Shirts_2008} established MBAR from a more general version of Eq.~\eqref{eq:mbar free energy estimator}, put forward by Kong \textit{et al}.,\cite{Kong_2003} in which a function $q_i(x)$ and a constant $\hat c_i$ replace the terms $e^{-u_i(x)}$ and $e^{-\hat f_i}$, respectively. It is
\begin{equation}
\label{eq:mbar general estimator}
{\hat c}_i = \sum_{k=1}^n \frac{q_i(x_k)}{\sum_{j=1}^m n_j q_j(x_k) c_j^{-1}}.
\end{equation}
 
This opens up the possibility of releasing some function $q_i(x)$ from being strictly non-negative, provided that $n_i = 0$ because it would not be an actual probability density. Kong \textit{et al}.\cite{Kong_2003} also developed an expression, valid for iid samples, for estimating a matrix whose each element $\hat \Theta_{i,j}$ is the asymptotic covariance between $\ln \hat c_i$ and $\ln \hat c_j$ obtained after solving the estimating equations self-consistently. Such expression is
\begin{equation}
\label{eq:mbar covariance matrix}
\hat{\mt \Theta} = \tr{\mt W} (\mt I - {\mt W}{\mt N}\tr{\mt W})^\dag {\mt W},
\end{equation}
where $\mt W$ is an $n \times m$ matrix with $W_{k,i} = \frac{q_i(x_k) \hat c_i^{-1}}{\sum_{j=1}^m n_j q_j(x_k) \hat c_j^{-1}}$, $\mt I$ is the $n \times n$ identity matrix, and $\mt N = \text{diag}(\vt n)$, where $\text{diag}(\cdot)$ returns a diagonal matrix built from the entries of a vector. The superscript $\dag$ denotes a Moore-Penrose pseudoinverse, required because the expression in parentheses produces a singular matrix. In fact, this can possibly be a very large matrix, but this might not become an unsurmountable issue in most cases (see Ref.~\onlinecite{Shirts_2008} for details). From the covariance matrix we can obtain, via the delta method, the square-error $\delta^2 h$ of any property $h$ that depends analytically on the set $\{\ln c_i\}_{i=1}^m$.
 
MBAR is also suitable for computing, via reweighting, ensemble averages and functions thereof (which includes relative free-energies), as well as their uncertainties, at both sampled states and unsampled ones. In a comment attached to Ref.~\onlinecite{Kong_2003}, Doss suggested a way of exploiting the flexibility of the $q$-functions in Eqs.~\eqref{eq:mbar general estimator} and \eqref{eq:mbar covariance matrix} in order to compute a property and its uncertainty at an unsampled state $m+1$, for which $q_{m+1}(x) = e^{u_{m+1}(x)}$. It consists in setting, in addition, a fictitious state $m+2$ for which $q_{m+2}(x) = A(x)e^{u_{m+1}(x)}$. In this way, Eq.~\eqref{eq:mbar general estimator} can be used to compute $c_{m+1}$ and $c_{m+2}$, from which one gets both $\hat f_{m+1} = -\ln c_{m+1}$ and $\bar A = {c_{m+2}}/{c_{m+1}}$. Because $n_{m+1} = n_{m+2} = 0$, such computation does not require iterations in case the $c$-constants of the sampled states have already been determined. The square-errors $\delta^2 \hat f_{m+1}$ and $\delta^2 \overline A$ then follow directly from Eq.~\eqref{eq:mbar covariance matrix} and the delta method, Eq.~\eqref{eq:delta method}.

\bibliography{mics}

\end{document}
