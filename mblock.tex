\documentclass[aip,jcp,reprint,amsmath,amssymb]{revtex4-1}
%\documentclass[aip,jcp,preprint,amsmath,amssymb]{revtex4-1}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[pdftex]{graphicx}

\newcommand{\mt}[1]{\boldsymbol{\mathbf{#1}}}           % matrix symbol
\newcommand{\vt}[1]{\boldsymbol{\mathbf{#1}}}           % vector symbol
\newcommand{\tr}[1]{#1^\text{t}}                        % transposition
\newcommand{\diff}[2]{\frac{\partial #1}{\partial #2}}  % partial derivative

\begin{document}

\title{Reweighting and Free Energy Computation Using Autocorrelated Samples From Multiple Equilibrium States}

\author{Charlles R. A. Abreu}
\email{abreu@eq.ufrj.br}
\affiliation{Chemical Engineering Department, Escola de Quimica, Universidade Federal do Rio de Janeiro, Rio de Janeiro, RJ 21941-909, Brazil}

\date{\today}

\maketitle

\section{Introduction}
\label{sec:introduction}

The purpose of this paper is twofold. First, we show that MBAR is intimately related to the expanded ensemble method. Second, and most importantly, 

\section{Definitions}
\label{sec:definitions}

Consider that $x$ represents the set of coordinates of a system that can be found at different equilibrium states. Each state $i$ is a statistical ensemble distinguished by a probability density
\begin{equation}
\label{eq:state_prob_density}
\rho_i(x) = \frac{1}{Z_i} e^{-u_i(x)},
\end{equation}
where $u_i(x)$ is a \textit{reduced potential}\cite{Shirts_2008, Chodera_2011} with known functional form, while $Z_i = \int e^{-u_i(x)}dx$ is the configurational integral of the system at state $i$. The ensemble average of a vector property $A(x)$ at state $i$ is defined as
\begin{equation}
\label{eq:ensemble average}
\langle A \rangle_i = \int A(x)\rho_i(x)dx.
\end{equation}

Assume that $n_i$ configurations $\{x_{i,k}\}_{k=1}^{n_i}$ were obtained via importance sampling\cite{Allen_1987} (e.g. MC or MD) at state $i$. In this case, we can estimate $\langle A \rangle_i$ by a simple arithmetic average, that is,
\begin{equation}
\label{eq:average_estimator}
\overline A_i = \frac{1}{n_i} \sum_{k=1}^{n_i} A(x_{i,k}).
\end{equation}

We are often interested in the difference between the reduced free energies of two states, defined as $\Delta f_{ij} = f_j - f_i = - \ln (Z_j/Z_i)$. As it is usually unfeasible to compute absolute free energies, we find appropriate to define $f_i$ as a relative value with respect to a state~$0$, chosen as a common reference to all other states of interest, so that $f_i = -\ln (Z_i/Z_0)$. We can thus rewrite Eq.~\eqref{eq:state_prob_density} as
\begin{equation}
\label{eq:state_prob_density_Z0}
\rho_i(x) = \frac{1}{Z_0} e^{-u_i(x)+ f_i}.
\end{equation}

The fact that $f_i$ can only be determined up to an additive constant will be assumed henceforth. One can estimate $f_i$ by relating it to one or more ensemble averages. We must remark, however, that importance sampling usually overlooks the tails of $\rho_i(x)$, meaning that only a limited region $C_i$ of the configurational space is sampled with reasonable accuracy. As in Ref.~\onlinecite{Jarzynski_2006}, we refer to $C_i$ as the set of \textit{typical} configurations of state $i$. It does not necessarily coincide with the set $D_i$ of configurations that substantially contribute to the integral in Eq.~\eqref{eq:ensemble average}, referred to as the \textit{dominant} configurations. In fact, the estimator in Eq.~\eqref{eq:average_estimator} will be accurate only if $D_i$ is a significant subset of $C_i$. This issue is particularly important in the calculation of free-energy differences and their uncertainties.

\section{Traditional Free-Energy Computation and Property Reweighting Methods}

With well-known methods such as Free Energy Perturbation (FEP),\cite{Zwanzig_1954} Simple Overlap Sampling,\cite{Lee_1980, Lu_2003} and Bennett Acceptance Ratio (BAR),\cite{Bennett_1976} free-energy differences can be computed for the pairs of states whose sets of typical configurations overlap most considerably and then pieced together to provide differences between poorly- or non-overlapping states. This strategy is known as \textit{staging}.\cite{Kofke_1998} Among the cited methods, FEP is the least recommended one. Given two states $i$ and $j$, it consists in evaluating either $\Delta f_{ij} = -\ln \langle e^{u_i - u_j} \rangle_i$ or $\Delta f_{ij} = \ln \langle e^{u_j - u_i} \rangle_j$.\cite{Zwanzig_1954} It happens, however, that these averages entail dominant sets $D_i$ and $D_j$ that practically coincide with the typical sets $C_j$ and $C_i$, respectively, which is opposite to the situation that would maximize accuracy.\cite{Jarzynski_2006} On the other hand, BAR is the most accurate method because it manages, in an iterative way, to relate $\Delta f_{ij}$ to ensemble averages computed at states $i$ and $j$ whose dominant sets $D_i$ and $D_j$ fulfill the intersection between $C_i$ and $C_j$.

Another way of tackling overlap deficiency consists in defining a reference state with a contiguous set $C_0$ that encompasses all sets $C_i$ associated to a number of states of interest. Then, instead of sampling configurations at these states, we do it at state $0$, with probability density $\rho_0(x) = \frac{1}{Z_0} e^{-u_0(x)}$, and compute every $f_i$ by means of the perturbation formula
\begin{equation}
\label{eq:nbs sampling free energy}
f_i = -\ln \langle e^{u_0-u_i} \rangle_0.
\end{equation}

As explained above, the dominant set $D_0$ will coincide with $C_i$, thus being a subset of $C_0$, as desired. Torrie and Valleau\cite{Torrie_1977} pioneered this idea, known as \textit{non-Boltzmann sampling} due to the usually non-physical character of the sampled state. One is also able to compute, directly from the configurations sampled at state $0$, ensemble averages of any property $A$ at the states of interest. This procedure, known as \textit{reweighting}, is based on the identity\cite{Torrie_1977}
\begin{equation}
\label{eq:nbs sampling reweighting}
\langle A \rangle_i = \frac{\langle A e^{u_0 - u_i} \rangle_0}{\langle e^{u_0 - u_i} \rangle_0} = \langle A e^{u_0 - u_i + f_i} \rangle_0.
\end{equation}

The multicanonical approach\cite{Berg_1992, Lee_1993, Abreu_2006} and the enveloping distribution method\cite{Christ_2007, *Christ_2008, *Christ_2009} are examples of successful non-Boltzmann sampling techniques. One of particular importance to the train of thought we intend to develop here is the method of \textit{expanded ensembles}.\cite{Lyubartsev_1992} It is a non-Boltzmann sampling scheme closely related to staging because it consists in combining a number $m$ of states whose individual distributions overlap. However, instead of generating independent samples, the sampling procedure includes a random walk through states. The joint probability of configurations and states is then given by\cite{Nymeyer_2010}
\begin{equation}
\label{eq:expanded ensemble joint}
p_0(x, i) = \frac{1}{Z_0} e^{-u_i(x) + \eta_i},
\end{equation}
where $\eta_i$ is part of an arbitrary vector $\vt \eta$ of weighting factors. Once $\vt \eta$ is defined, there are several ways of performing importance sampling from $p_0(x,i)$, which will result in a sample $S = \{x_k,i_k\}_{k=1}^n$, with each configuration bearing a state-indicating label. For completeness, in Appendix \ref{sec:expanded ensemble simulation} we describe, in brief, some common sampling schemes. From Eq.~\eqref{eq:expanded ensemble joint}, one can derive the marginal probability of each state $i$, defined as $\pi_i = \int p_0(x,i)dx$, as well as the marginal probability density at state~$0$, defined as $\rho_0(x) = \sum_{i=1}^m p_0(x,i)$. One can also obtain the conditional probabilities $p(i|x)$ and $p(x|i)$ via Bayes' theorem, $p(x,i) = p(x|i) \pi_i = p(i|x) \rho_0(x)$. Hence,
\begin{subequations}
\label{eq:expanded ensemble probabilities}
\begin{gather}
\pi_i = \frac{Z_i}{Z_0} e^{\eta_i} = e^{\eta_i - f_i}, \label{eq:expanded ensemble prior} \\
\rho_0(x) = \frac{1}{Z_0} \sum_{j=1}^m e^{-u_j(x) + \eta_j}, \label{eq:expanded ensemble evidence} \\
p(i|x) = \frac{e^{-u_i(x) + \eta_i}}{\sum_{j=1}^m e^{-u_j(x) + \eta_j}}, \; \text{and} \label{eq:expanded ensemble posterior} \\
p(x|i) = \frac{1}{Z_0} e^{u_i(x) + f_i}. \label{eq:expanded ensemble likelihood}
\end{gather}
\end{subequations}

The fact that $p(x|i) = \rho_i(x)$ demonstrates that each individual state is sampled correctly. Once the sample $S$ has been generated, a simple estimator for the relative free energy of each state stems from Eq.~\eqref{eq:expanded ensemble prior} by making $\hat \pi_i = \frac{n_i}{n}$, where $n_i$ is the number of appearances of state $i$ in $S$. The result is
\begin{equation}
\label{eq:expanded ensemble histogram estimator}
\hat f_i = \eta_i - \ln \frac{n_i}{n}.
\end{equation}

However, we can obtain a better estimator if we insert $u_0 = -\ln(Z_0\rho_0)$ into Eq.~\eqref{eq:nbs sampling free energy}, with $\rho_0$ given by Eq.~\eqref{eq:expanded ensemble evidence}, and then use Eq.~\eqref{eq:average_estimator} to compute the required ensemble average. Such estimator is
\begin{equation}
\label{eq:expanded ensemble FEP estimator}
\hat f_i = -\ln \left[ \frac{1}{n}\sum_{k=1}^n \frac{e^{-u_i(x_k)}}{\sum_{j=1}^m e^{-u_j(x_k) + \eta_j}} \right].
\end{equation}

Note that the state-indicating labels in $S$ are neglected in both estimators, due to an essential feature of importance sampling in that every sampled configuration contributes equally to an ensemble average. In addition, we remark that the set of weighting factors is often chosen so that all states are visited with equal frequency. For this to occur, we must impose that $\eta_i = f_i - \ln m$, which can only be achieved iteratively. Other strategies have also been proposed to select suitable weighting factors or reduced potentials that improve performance and avoid ergodicity issues. Details can be found in Refs.~\onlinecite{Katzgraber_2006, Trebst_2006, Escobedo_2007, *Escobedo_2008, *Martinez_2008}.

Finally, we combine the results in Eq.~\eqref{eq:expanded ensemble probabilities} to show that
\begin{equation}
\label{eq:mixture ensemble}
\rho_0(x) = \sum_{i=1}^m \pi_i \rho_i(x),
\end{equation}
meaning that an expanded ensemble distribution is actually a \textit{mixture model}\cite{Lindsay_1995, Marin_2005} composed of the distributions corresponding to the $m$ states. In this context, the vector $\vt \pi$ of state probabilities can be referred to as the mixture composition. In addition, since $\rho_0 = \frac{1}{Z_0}e^{-u_0}$, we can resort to Eq.~\eqref{eq:state_prob_density_Z0} for expressing the reduced potential of the mixture state as
\begin{equation}
\label{eq:mixture potential}
u_0(x) = -\ln \sum_{i=1}^m \pi_i e^{-u_i(x) + f_i}.
\end{equation}

With $\vt f$ replaced by its estimator $\hat{\vt f}$, this potential can be employed together with Eqs.~\eqref{eq:nbs sampling free energy} and \eqref{eq:nbs sampling reweighting} to estimate relative free energies and other ensemble averages at any state $i$ whose set $C_i$ is a subset of $C_0$, whether such state is part of the expanded ensemble ($1 \le i \le m$) or not.

\section{The Multistate Bennett Acceptance Ratio Method}

An interesting feature of the method of expanded ensembles is that the walk through the multiple states favors ergodicity in each state, as compared to sampling each one individually. Nevertheless, the process of determining convenient states and weights can be tedious and time-consuming. It is more likely that we face the situation described in Sec.~\ref{sec:definitions}, when samples individually drawn at different states are available. An advantage of this scenario is that inclusion of new states, if necessary, is straightforward. In this case, the pooled sample can be expressed as
\begin{equation}
\label{eq:pooled sample}
\mathcal S = \Big\{\{x_{i,k}\}_{k=1}^{n_i}\Big\}_{i=1}^m.
\end{equation}

Shirts and Chodera\cite{Shirts_2008} described a method for computing relative free energies and their uncertainties by using all configurations in $\mathcal S$, provided that each sample is composed of independent and identically distributed (iid) configurations. The authors named the method as Multistate Bennet Acceptance Ratio (MBAR) because it is equivalent to BAR when only two states are involved. Let us derive their estimator from a simple argument before embarking in a detailed analysis. MBAR consists in assigning the same importance to every configuration, regardless of which state it comes from. We can thus overlook the subset structure of Eq.~\eqref{eq:pooled sample} and write $\mathcal S = \{x_k\}_{k=1}^n$. This is equivalent to viewing $\mathcal S$ as the outcome of an expanded ensemble simulation whose weighting factors are unknown and must be estimated as well as the relative free energies. For this reason, we are compelled to use both estimators in Eqs.~\eqref{eq:expanded ensemble histogram estimator} and \eqref{eq:expanded ensemble FEP estimator}, rather than choosing one of them. A combination of these estimators yield
\begin{equation}
\label{eq:mbar free energy estimator}
\hat f_i = -\ln \sum_{k=1}^n \frac{e^{-u_i(x_k)}}{\sum_{j=1}^m n_j e^{-u_j(x_k) + \hat f_j}},
\end{equation}
which is equivalent to Eq.~11 of Ref.~\onlinecite{Shirts_2008}. Note that this is not an isolated estimator, but a self-consistent system of equations whose solution provides the whole vector $\hat {\vt f}$. In fact, because the solution can only be determined up to an additive constant, one can fix $\hat f_1$ and solve the remaining equations in order to obtain $\{\hat f_i\}_{i=2}^m$. Once the solution has been found, we can compute the potential $u_0$ that defines the mixture distribution $\rho_0(x)$ via Eq.~\eqref{eq:expanded ensemble potential} with $\eta_j = \hat f_j + \ln \frac{n_j}{n}$. This enables us to employ Eqs.~\eqref{eq:nbs sampling free energy} and \eqref{eq:nbs sampling reweighting} in order to perform FEP and/or reweighting calculations from the mixture to individual states (either sampled or unsampled ones).\cite{Geyer_1994, Shirts_2017}

As the $m$ samples in $\mathcal S$ are generated independently, giving all configurations the same importance seems unsuitable if these samples consist of correlated time series with distinct mixing times. This is one reason why MBAR is meant to deal with uncorrelated data and, as such, requires subsampling if the series obtained at any state is originally correlated.\cite{Shirts_2008} For a series $\{x_{i,k}\}_{k=1}^{n_i}$ drawn at state $i$, the subsampling interval should be set as the statistical inefficiency of some configurational property (e.g. the reduced potential $u_i$). The statistical inefficiency of a property $A(x)$ can be computed as\cite{Chodera_2007}
\begin{equation}
\label{eq:statistical inefficiency}
g_i = \left\lceil 1 + 2 \sum\limits_{t=1}^{n_i-1} \frac{n_i - t}{n_i} \alpha_i(t) \right\rceil,
\end{equation}
where $\lceil \cdot \rceil$ is the ceiling function and $\alpha_i(t)$ is the normalized autocorrelation function of $A$, calculated by
\begin{equation*}
\alpha_i(t) = \frac{\dfrac{1}{n_i - t} \sum\limits_{k=1}^{n_i-t} [A(x_{i,k}) - \overline A_i][A(x_{i,k+t}) - \overline A_i]}{\dfrac{1}{n_i} \sum\limits_{k=1}^{n_i} [A(x_{i,k}) - \overline A_i]^2}.
\end{equation*}

Ideally, one should compute $g_i$ for a number of relevant properties and take the largest one as the subsampling interval.\cite{Shirts_2008} The autocorrelation function $\alpha_i(t)$ is expected to asymptotically decay to zero for most properties, but in practice its tail fluctuates around zero. Chodera \textit{et al}.\cite{Chodera_2007} proposed that the sum in Eq.~\eqref{eq:statistical inefficiency} should be truncated right before $\alpha_i(t)$ crosses zero for the first time, arguing that the function becomes statistically indistinguishable from zero at this point.

The most important reason why MBAR requires uncorrelated data is concerned with the determination of uncertainties. The estimator has appeared in the literature under other denominations such as Biased Sampling,\cite{Vardi_1985, *Gill_1988} Reverse Logistic Regression,\cite{Geyer_1994} and Extended Bridge Sampling.\cite{Meng_1996, Kong_2003, Tan_2004} It has been derived in several ways from the principle of maximum likelihood, which can lead to standardized proofs of central limit theorems (CLT's), thus providing reliable estimates of asymptotic covariance matrices.\cite{Pawitan_2001, Greene_2012} These proofs, however, often follow from the premise that iid samples are available. Shirts and Chodera\cite{Shirts_2008} established MBAR from a more general version of Eq.~\eqref{eq:mbar free energy estimator}, put forward by Kong \textit{et al}.,\cite{Kong_2003} in which a function $q_i(x)$ and a constant $\hat c_i$ replace the terms $e^{-u_i(x)}$ and $e^{-\hat f_i}$, respectively. That is,
\begin{equation}
\label{eq:kong et al estimator}
\hat c_i = \sum_{k=1}^n \frac{q_i(x_k)}{\sum_{j=1}^m n_j q_j(x_k) \hat c_j^{-1}}.
\end{equation}

This opens up the possibility of releasing some function $q_i(x)$ from being strictly non-negative, provided that $n_i = 0$ because it would not be an actual probability density. Kong \textit{et al}.\cite{Kong_2003} also developed an expression, valid for iid samples, for estimating a matrix whose each element $\hat \Theta_{i,j}$ is the asymptotic covariance between $\ln \hat c_i$ and $\ln \hat c_j$ obtained after solving Eq.~\eqref{eq:kong et al estimator} self-consistently. Such expression is
\begin{equation}
\label{eq:mbar covariance matrix}
\hat{\mt \Theta} = \tr{\mt W}(\mt I - {\mt W}{\mt N}\tr{\mt W})^+{\mt W},
\end{equation}
where $\mt W$ is an $n \times m$ matrix with $W_{k,i} = \frac{q_i(x_k) \hat c_i^{-1}}{\sum_{j=1}^m n_j q_j(x_k) \hat c_j^{-1}}$, $\mt I$ is the $n \times n$ identity matrix, and $\mt N$ is an $m \times m$ diagonal matrix with $N_{i,i} = n_i$. The superscript $+$ denotes a Moore-Penrose pseudoinverse, required because the expression in parentheses results in a singular matrix. We remark that the potential issue of computing the pseudoinverse of a very large matrix can be mitigated in most cases (see Ref.~\onlinecite{Shirts_2008} for details). From the covariance matrix, we can use the delta method\cite{Greene_2012} to estimate the squared error $\delta^2 Y$ of any property $Y$ that depends analytically on the set $\{\ln c_i\}_{i=1}^m$. In the case of a free energy difference $\Delta \hat f_{i,j}$, we have
\begin{equation}
\label{eq:mbar free mean-square error}
\delta^2 \Delta {\hat f_{i,j}} = {\hat \Theta}_{i,i} - 2 {\hat \Theta}_{i,j} + {\hat \Theta}_{j,j}.
\end{equation}

In the case of a perturbation calculation to compute the relative free energy of an unsampled state with reduced energy $u(x)$,

In a comment attached to Ref.~\onlinecite{Kong_2003}, Doss suggested a way of exploring this possibility in order to estimate the uncertainties of not only free-energy differences, but also other ensemble averages given by Eq.~\eqref{eq:nbs sampling reweighting}. His suggestion requires estimating a matrix $\hat {\mt \Theta}$ whose each element $\hat \Theta_{i,j}$ is the asymptotic covariance between $\ln \hat c_i$ and $\ln \hat c_j$ obtained after solving Eq.~\eqref{eq:kong et al estimator} self-consistently. In MBAR, this is done by using the formula proposed by Kong \textit{et al}.,\cite{Kong_2003} which relies on the assumption of iid samples. It is
\begin{equation*}
\frac{\delta^2 \overline A}{\overline A^2} = {\hat \Theta}_{m+1,m+1} - 2 {\hat \Theta}_{m+1,m+2} + {\hat \Theta}_{m+2,m+2}
\end{equation*}

\section{Taking Full Advantage of Correlated Data from Multiple States}

The requirement of subsampling often causes MBAR to waste a considerable amount of data. Even though the discarded configurations carry less information than do the ones that remain, they could possibly have a positive impact on the quality of a statistical analysis. Here we develop an unwasteful extension of MBAR, which allows us to use all available data even if individual samples exhibit distinct mixing times.

\subsection{Relative Free Energies of Sampled States}

The starting point of our proposal is an exact expression obtained from Eqs.~\eqref{eq:nbs sampling free energy} and \eqref{eq:mixture potential}, which is
\begin{equation}
\label{eq:free energy exact}
f_i = -\ln \left\langle \frac{e^{-u_i}}{\sum_{j=1}^m \pi_j e^{-u_j + f_j}} \right\rangle_0.
\end{equation}

In MBAR, the average above is estimated by applying Eq.~\eqref{eq:average_estimator} to the pooled sample $\mathcal S$ as a whole. This decision is the cause of all restraints discussed in the preceding section, since all configurations in $\mathcal S$ must be identically distributed according to Eq.~\eqref{eq:mixture ensemble}, so that both $\vt \pi$ and $\vt f$ can be properly determined. Nevertheless, one can show via Eqs.~\eqref{eq:ensemble average} and \eqref{eq:mixture ensemble} that, for a mixture model,
\begin{equation}
\label{eq:mixture average}
\langle A \rangle_0 = \sum_{i=1}^m \pi_i \langle A \rangle_i.
\end{equation}

Therefore, we can compute an average at the mixture state from averages at the individual ones, that is,
\begin{equation}
\label{eq:mixture average estimator}
{\overline A}_0 = \sum_{i=1}^m \pi_i \overline{A}_i = \sum_{i=1}^m \frac{\pi_i}{n_i} \sum_{k=1}^m A(x_{i,k}).
\end{equation}

This requires no assumption on the distribution of the pooled sample as a whole. Moreover, the estimator is valid regardless of the mixture composition $\vt \pi$, which can thus be specified beforehand, leaving only the relative free energies to be estimated. Evidently, the choice of $\vt \pi$ might affect the asymptotic behavior of the estimator and this will be a topic of discussion shortly. The new estimating equation for the relative free energies, which emerges from Eqs.~\eqref{eq:average_estimator}, \eqref{eq:free energy exact}, and \eqref{eq:mixture average}, is
\begin{equation}
\label{eq:mblock free energy estimator}
{\hat f}_i = -\ln \sum_{j=1}^m \frac{\pi_j}{n_j} \sum_{k=1}^{n_j} \frac{e^{-u_i(x_{j,k})}}{\sum_{s=1}^m \pi_s e^{-u_s(x_{j,k}) + {\hat f}_s}}.
\end{equation}

As with the MBAR estimator, we again have a system of equations that must be solved self-consistently after setting, for instance, the value of $\hat f_1$. In contrast to that case, however, the state at which each configuration was sampled is now a relevant information, unless we specify $\pi_j = {n_j}/{n}$ in order to recover Eq.~\eqref{eq:mbar free energy estimator}. Doss and Tan\cite{Doss_2014} proposed an equivalent estimator and investigated its asymptotic behavior under some special conditions. In a subsequent study, Roy \textit{et al}.\cite{Roy_2018} established a CLT applicable when each individual sample originates from a Markov process with polynomial-rate convergence and can be split into uncorrelated blocks of equal size. Here we assume the validity of these conditions and consider that the statistical inefficiency $g_i$, computed from Eq.~\eqref{eq:statistical inefficiency}, is a sufficiently large block size for the sample obtained at a state $i$. Therefore, such sample is split into a number of blocks given by
\begin{equation*}
\label{eq:mblock number of blocks}
%n^B_i = \left\lfloor \frac{n_i + g_i - 1}{g_i} \right\rfloor,
n^B_i = \left\lfloor \frac{n_i}{g_i} \right\rfloor,
\end{equation*}
where $\lfloor \cdot \rfloor$ is the floor function. One usually refers to $n^B_i$ as an \textit{effective sample size}. As Roy \textit{et al}.\cite{Roy_2018} suggested and our results will confirm, a suitable specification for the marginal probability of each state $i$ is
\begin{equation}
\label{eq:mblock prior}
\pi_i = \frac{n^B_i}{\sum_{j=1}^m n^B_j}.
\end{equation}

The assumption of uncorrelated blocks will be crucial for determining the asymptotic uncertainty of the estimator in Eq.~\eqref{eq:mblock free energy estimator}. Henceforth, the proposed multistate blocking method will be referred to as MBLOCK.

\subsection{Maximum Likelihood Approach and Numerical Computation}

The non-linear system in Eq.~\eqref{eq:mblock free energy estimator} is a maximum likelihood (ML) estimator that can be derived from the following log-quasi-likelihood function:\cite{Doss_2014, Tan_2015, Roy_2018}
\begin{equation}
\label{eq:mblock log-quasi-likelihood}
\ln \mathcal L = \sum_{k=1}^m \frac{\pi_k}{n_k} \sum_{l=1}^{n_k} \ln p_k(x_{k,l}),
\end{equation}
where $p_i(x)$ is the conditional probability $p(i|x)$, obtained by combining Eqs.~\eqref{eq:expanded ensemble prior} and \eqref{eq:expanded ensemble posterior}, that is,
\begin{equation}
\label{eq:mixture posterior probability}
p_i(x) = \frac{\pi_i e^{-u_i(x) + f_i}}{\sum_{j=1}^m \pi_j e^{-u_j(x) + f_j}}.
\end{equation}

It is now convenient to introduce a vector/matrix notation. We start by defining a vector-valued configurational property $\vt p(x)$, whose each entry is given by Eq.~\eqref{eq:mixture posterior probability}, and a matrix-valued property $\mt \Omega(x) = {\vt p}(x)\tr{\vt p}(x)$, where $\tr{\vt p}$ is the transpose of $\vt p$. Note that Eqs.~\eqref{eq:average_estimator} and \eqref{eq:mixture average estimator} naturally extend to multivalued properties, so that we can use them to compute averages like $\overline{\vt p}_i$, $\overline{\mt \Omega}_i$, $\overline{\vt p}_0$, and $\overline{\mt \Omega}_0$. Let us introduce $\vt g(\vt f) = - \nabla_{\vt f} \ln \mathcal L$ and $\mt B(\vt f) = \nabla_{\vt f} \vt g$, thus the reciprocal of the gradient and the Hessian of the log-quasi-likelihood function with respect to vector $\vt f$, respectively. The first- and second-order derivatives of $\ln p_k(x)$ with respect to $f_i$ and $f_j$ are
\begin{equation*}
\frac{\partial \ln p_k}{\partial f_i} = \delta_{i,k} - p_i \quad \text{and} \quad \frac{\partial^2\ln p_k}{\partial f_i \partial f_j} = -p_i(\delta_{i,j} - p_j),
\end{equation*}
where $\delta_{i,j}$ is the Kronecker delta. Finally, these results can be applied together with Eqs.~\eqref{eq:mblock log-quasi-likelihood} and \eqref{eq:mixture average estimator} to yield
\begin{align}
\vt g(\vt f) &= \overline{\vt p}_0 - \vt \pi \quad \text{and} \\
\mt B(\vt f) &= \text{diag}(\overline{\vt p}_0) - \overline{\mt \Omega}_0,
\end{align}
where $\text{diag}(\cdot)$ returns a diagonal matrix built from the entries of a vector. For a given pooled sample $\mathcal S$ and predefined mixture composition $\vt \pi$, both $\vt g$ and $\mt B$ are exclusive functions of the free-energy vector $\vt f$. The MLE $\hat{\vt f}$ is thus the solution of $\vt g = \vt 0$, which occurs when
\begin{equation}
\label{eq:mblock estimator in matrix form}
\overline{\vt p}_0 = \vt \pi,
\end{equation}
that is, when the computed expectation of $\vt p(x)$ in the mixture state equals the specified mixture composition. The equation above is just a reformulation of Eq.~\eqref{eq:mblock free energy estimator}, given that $\overline{\vt p}_0$ is defined as in Eq.~\eqref{eq:mixture average estimator}. Another interpretation for the ML estimator $\hat{\vt f}$ is possible by defining a stochastic matrix $\mt O = [\begin{array}{ccc} \overline{\vt p}_1 & \cdots & \overline{\vt p}_m \end{array}]$, known as the \textit{overlap matrix}, whose applicability in the quantification of phase-space overlap in multistate methods was analyzed in Ref.~\onlinecite{Klimovich_2015}. Its definition allows us to rewrite Eq.~\eqref{eq:mblock estimator in matrix form} as
\begin{equation}
\mt O \vt \pi = \vt \pi,
\end{equation}
meaning that the ML estimator $\hat{\vt f}$ is the one for which the overlap matrix $\mt O$ has the mixture composition $\vt \pi$ as its stationary distribution, which is guaranteed to exist if $\mt O$ is ergodic.

Newton-Raphson:
\begin{equation}
\label{eq:mblock Newton-Raphson}
{\vt f}_{[2:m]} \leftarrow {\vt f}_{[2:m]} - \vt \phi,
\end{equation}
where $\vt \delta$ is the solution of the linear system
\begin{equation}
{\mt B}_{[2:m,2:m]} {\vt \phi} = {\vt g}_{[2:m]}
\end{equation}

\subsection{Uncertainty of Relative Free Energies}

Considering that all individual samples are independent:
\begin{equation}
\text{cov}(\overline{\vt y}_0,\overline{\vt z}_0) = \sum_{i=1}^m \pi_i^2 \text{cov}(\overline{\vt y}_i,\overline{\vt z}_i).
\end{equation}

Batch mean (block) estimator:\cite{Geyer_1992}
\begin{equation}
\text{cov}(\overline{\vt y}_0,\overline{\vt z}_0) = \sum_{i=1}^m  \frac{\pi_i^2}{n^B_i(n^B_i - 1)} \sum\limits_{j=1}^{n^B_i} (\overline{\vt y}^B_{i,j} - \overline{\vt y}_i) \tr{(\overline{\vt z}^B_{i,j} - \overline{\vt z}_i)}.
\end{equation}

The average of vector-valued property $\vt a(x)$ for the $j$-th block of the sample at state $i$ is
\begin{equation}
\overline{\vt y}_{i,j} = \frac{1}{g_i} \sum_{k=1}^{g_i} {\vt y}\left(x_{i,g_i(j-1)+k}\right),
\end{equation}



\begin{equation}
\nabla_{\vt f}\overline{\vt p}_0 = \mt B.
\end{equation}

\begin{gather}
\overline{\vt z}_0 = \sum_{i=1}^m \pi_i \overline{\vt z}_i \\
\mt \Theta_{[\overline{\vt a}_0,\overline{\vt b}_0]} = \sum_{i=1}^m \pi_i^2 \text{cov}(\overline{\vt a}_i,\overline{\vt b}_i)
\end{gather}



\subsection{Free-Energy Perturbation and Reweighting}

\begin{equation*}
\langle e^{u_0 - u} \rangle = \sum_{i=1}^m \frac{\pi_i}{n_i}\sum_{j=1}^{n_i} 
\end{equation*}

\section{Results}

\subsection{Propane Solvation in Water}

In order to compare the proposed method against MBAR in regard to the calculation of relative free energies of sampled states, we simulated the infinite-dilution solvation of propane in water. Both molecules are modeled as rigid bodies. The intermolecular interactions between water atoms and propane pseudo-atoms are calculated by the softcore potential\cite{Beutler_1994} $\nu(r,\lambda) = 4\lambda\epsilon(s^{-2} - s^{-1})$, where $s = (r/\sigma)^6 + 0.5 (1-\lambda)$.

\begin{table}
\caption{Reduced relative free energies}
\label{table:propane solvation}
\begin{ruledtabular}
\begin{tabular}{ccccc}
$\lambda$ & $g$ & MBAR & GMBAR & Optimized \\
\hline
$0.0$ & $12$ & $0.0$ & $0.0$ & $0.0$ \\
$0.1$ & $14$ & $0.4547 \pm 0.0025$ & $0.4563 \pm 0.0018$ \\
$0.2$ & $21$ & $1.7643 \pm 0.0073$ & $1.7668 \pm 0.0056$ \\
$0.3$ & $27$ &  $3.497 \pm 0.018$  &  $3.491 \pm 0.014$ \\
$0.4$ & $12$ &  $4.475 \pm 0.027$  &  $4.462 \pm 0.020$ \\
$0.5$ & $8$  &  $4.728 \pm 0.030$  &  $4.712 \pm 0.022$ \\
$0.6$ & $5$  &  $4.636 \pm 0.031$  &  $4.619 \pm 0.023$ \\
$0.8$ & $4$  &  $3.979 \pm 0.032$  &  $3.958 \pm 0.023$ \\
$1.0$ & $2$  &  $2.989 \pm 0.032$  &  $2.968 \pm 0.024$
\end{tabular}
\end{ruledtabular}
\end{table}

\begin{figure}
\centering
\includegraphics{Figures/nelder_mead}
\caption{}
\label{fig:nelder_mead}
\end{figure}


\section{Conclusion}

Free-energy perturbation is an accurate method when the typical configurations of the target state form a significant subset of the typical configurations of the sampled state.

We tried to interpret different methods simply as distinct choices of a reduced potential $u_0(x)$ for the reference state, expressed as an analytical function $u_0 = u_0(u_1,\dots,u_n)$.

\appendix

\section{Simulation of an Expanded Ensemble}
\label{sec:expanded ensemble simulation}

One can use it, for instance, in a MC simulation that involves both configurational and state-changing moves.\cite{Lyubartsev_1992} This will result in a sample $S = \{x_k,i_k\}_{k=1}^n$, with each configuration bearing a state-indicating label. Another way is via MC or MD of a system whose reduced potential is computed by
\begin{equation}
%\label{eq:expanded ensemble potential}
u_0(x) = - \ln \sum_{j=1}^m e^{-u_j(x) + \eta_j},
\end{equation}
which results in the probability density of Eq.~\eqref{eq:expanded ensemble evidence}. In this case, the sampled configurations will not bear any labels at first. However, if somehow these labels are needed, we can randomly pick a label for each configuration from the conditional distribution $p(i|x)$.\cite{Nymeyer_2010} From Bayes' theorem $p(x,i) = p(x|i) p(i) = p(i|x) p(x)$, we can divide Eq.~\eqref{eq:expanded ensemble joint} by Eq.~\eqref{eq:expanded ensemble evidence} to find out that
\begin{equation}
%\label{eq:expanded ensemble posterior}
p(i|x) = \frac{e^{-u_i(x) + \eta_i}}{\sum_{j=1}^m e^{-u_j(x) + \eta_j}}.
\end{equation}

In addition, we can also divide Eq.~\eqref{eq:expanded ensemble joint} by Eq.~\eqref{eq:expanded ensemble prior} to see how configurations are distributed within each state. The results is $p(x|i) = Z_0^{-1} e^{u_i(x) + f_i}$, which is identical to Eq.~\eqref{eq:state_prob_density_Z0} and proves that each state of the expanded ensemble is sampled correctly. Together with Bayes' theorem, this can also be used to show that
\begin{equation}
%\label{eq:mixture ensemble}
\rho_0(x) = \sum_{i=1}^m \pi_i \rho_i(x),
\end{equation}
which means that an expanded ensemble distribution is actually a \textit{mixture model}\cite{Lindsay_1995, Marin_2005} composed of the distributions at all states. The method proposed by Nymeyer\cite{Nymeyer_2010} and a special case of the one devised by Christ and van Gunsteren\cite{Christ_2007, *Christ_2008, *Christ_2009} are examples of using Eq.~\eqref{eq:expanded ensemble potential} to perform non-Boltzmann sampling with MD. A third method for sampling from an expanded ensemble is known as \textit{Gibbs sampling}.\cite{Marin_2005, Chodera_2011} While the state $i$ is kept fixed, configurations can be sampled from $p(x|i)$ via MC or MD. Also, while keeping the configuration $x$ fixed, a state can be chosen according to $p(i|x)$. Alternating these procedures will produce the correct expanded ensemble distribution.\cite{Chodera_2011}

Once a sample $S$ has been generated, a simple estimator for the relative free energy of each state stems from Eq.~\eqref{eq:expanded ensemble prior} by making $\hat \pi_i = n_i/n$, where $n_i$ is the number of appearances of state $i$ in $S$. The result is
\begin{equation}
%\label{eq:expanded ensemble histogram estimator}
\hat f_i = \eta_i - \ln \frac{n_i}{n}.
\end{equation}

However, a better estimator can be obtained by combining Eqs.~\eqref{eq:nbs sampling free energy} and \eqref{eq:expanded ensemble potential} and then using Eq.~\eqref{eq:average_estimator} to compute the required ensemble average. It is
\begin{equation}
%\label{eq:expanded ensemble FEP estimator}
\hat f_i = -\ln \left[ \frac{1}{n}\sum_{k=1}^n \frac{e^{-u_i(x_k)}}{\sum_{j=1}^m e^{-u_j(x_k) + \eta_j}} \right].
\end{equation}

Both estimators neglect the detailed association of configurations to states. This is due to an essential feature of importance sampling, in that every sampled configuration contributes equally to an ensemble average. During the simulation of a complex system, it is a common practice to store only a small set of collective properties for posterior analysis. In this case, if the second estimator is to be used, one must ensure that such set includes all the potentials present in Eq.~\eqref{eq:expanded ensemble FEP estimator} or other properties from which they can be computed afterwards. Finally, we remark that the set of weighting factors is often chosen so that all states are visited with equal frequency. For this to occur, we must impose that $\eta_i = f_i - \ln m$, which can only be achieved iteratively. Other strategies have also been proposed to select suitable weighting factors or reduced potentials that improve performance and avoid ergodicity issues. Details can be found in Refs.~\onlinecite{Katzgraber_2006, Trebst_2006, Escobedo_2007, *Escobedo_2008, *Martinez_2008}.


\bibliography{mblock}

\end{document}
